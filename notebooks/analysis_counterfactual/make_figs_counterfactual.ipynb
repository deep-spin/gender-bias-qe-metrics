{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(\"paper\", style=\"whitegrid\", font=\"serif\", font_scale=2.4)\n",
    "mpl.rcParams[\"axes.grid\"] = True\n",
    "mpl.rcParams[\"grid.color\"] = \"lightgray\"\n",
    "mpl.rcParams[\"grid.linestyle\"] = \"--\"\n",
    "mpl.rcParams[\"grid.linewidth\"] = 0.5\n",
    "plt.rc(\"text\", usetex=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counterfactual dataset -- References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tables for continuous metrics QE ratio\n",
    "\n",
    "Results are computed with analysis_mt_geneval_counterfactual_all_models.ipynb and shown below. \n",
    "\n",
    "- QE(s_f,h_f) / QE(s_m,h_m)   aggregated over  languages\n",
    "- QE(s_f,h_f) / QE(s_m,h_m)     statistical test (1sample) per language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    \\begin{table*}\n",
      "    \\centering\n",
      "    \\small\n",
      "    \\begin{tabular}{lcccccccc}\n",
      "    \\toprule\n",
      "    \\rowcolor{white}\n",
      "    \\textbf{Metrics} & \\multicolumn{8}{c}{$QE(s_F,h_F) / QE(s_M,h_M)$ } \\\\\n",
      "    & de & es & fr & it & pt & ru & hi & ar \\\\\n",
      "    \\midrule\n",
      "    GPT 4 & 0.999 \\gagood & 1.003 \\gagood & 0.998 \\gagood & 1.003 \\gagood & 1.000 \\gagood & 0.993 \\gagood & 1.000 \\gagood & 1.007 \\gagood \\\\\n",
      "Gemma 2 9B & 0.997 \\gagood & 1.001 \\gagood & 1.002 \\gagood & 1.004 \\gagood & 0.999 \\gagood & 0.998 \\gagood & 1.003 \\gagood & 0.999 \\gagood \\\\\n",
      "Kiwi 22 & 0.995 \\gabad & 0.995 \\gabad & 0.995 \\gabad & 0.996 \\gabad & 1.000 \\gagood & 0.997 \\gabad & 0.998 \\gabad & 1.000 \\gagood \\\\\n",
      "\\addlinespace[0.2cm]\n",
      "Kiwi 23 XL & 0.997 \\gagood & 1.000 \\gagood & 0.998 \\gagood & 0.996 \\gagood & 0.998 \\gagood & 1.000 \\gagood & 1.006 \\gagood & 1.015 \\gagood \\\\\n",
      "Kiwi 23 XXL & 1.005 \\gagood & 0.995 \\gagood & 1.006 \\gagood & 0.991 \\gabad & 0.999 \\gagood & 1.001 \\gagood & 1.006 \\gagood & 1.025 \\gagood \\\\\n",
      "\\addlinespace[0.2cm]\n",
      "Llama 3.1 70B & 1.002 \\gagood & 0.993 \\gagood & 0.989 \\gabad & 0.999 \\gagood & 0.993 \\gagood & 0.991 \\gabad & 0.995 \\gagood & 1.001 \\gagood \\\\\n",
      "MetricX23 LARGE & 0.999 \\gagood & 0.998 \\gagood & 0.998 \\gabad & 0.998 \\gabad & 1.000 \\gagood & 0.999 \\gagood & 1.000 \\gagood & 0.992 \\gabad \\\\\n",
      "MetricX23 XL & 0.998 \\gabad & 0.998 \\gagood & 0.998 \\gagood & 1.000 \\gagood & 1.001 \\gagood & 1.000 \\gagood & 1.002 \\gagood & 1.000 \\gagood \\\\\n",
      "Mistral 7B & 1.000 \\gagood & 1.000 \\gagood & 0.999 \\gagood & 1.000 \\gagood & 0.998 \\gabad & 1.000 \\gagood & 1.002 \\gagood & 1.002 \\gagood \\\\\n",
      "xCOMET XL & 0.988 \\gabad & 0.975 \\gabad & 0.997 \\gagood & 0.979 \\gabad & 0.982 \\gabad & 0.991 \\gabad & 0.996 \\gagood & 0.995 \\gagood \\\\\n",
      "xCOMET XXL & 0.997 \\gabad & 0.993 \\gagood & 0.997 \\gagood & 0.989 \\gabad & 1.001 \\gagood & 0.998 \\gagood & 0.989 \\gabad & 0.995 \\gagood \\\\\n",
      "\n",
      "    \\bottomrule\n",
      "    \\end{tabular}\n",
      "    \\caption{\\textbf{MT-GenEval Counterfactual Results} on human-written translations. \n",
      "    A green check mark (\\cmark) indicates that correctly-inflected feminine references receive statistically significantly (p<0.05) lower scores than correctly-inflected masculine references. Numbers represent the ratio mean, and significance results are shown in parentheses.} \n",
      "    \\label{tab:contextual_stat_tests}\n",
      "    \\end{table*}\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "res_overall_analysis_path = './results-copied/stats/nonambiguous-counterfactual/references/continuous-analysis/results.csv'\n",
    "results_df_cont = pd.read_csv(res_overall_analysis_path)\n",
    "\n",
    "def create_significance_table_QE_ratio(df):\n",
    "    # Extract unique models and languages from the DataFrame\n",
    "    models = df['model'].unique()\n",
    "    languages = ['de', 'es', 'fr', 'it', 'pt', 'ru', 'hi', 'ar']\n",
    "\n",
    "    # Define the LaTeX table header\n",
    "    table_header = r\"\"\"\n",
    "    \\begin{table*}\n",
    "    \\centering\n",
    "    \\small\n",
    "    \\begin{tabular}{l\"\"\" + \"c\" * len(languages) + r\"\"\"}\n",
    "    \\toprule\n",
    "    \\rowcolor{white}\n",
    "    \\textbf{Metrics} & \\multicolumn{\"\"\" + str(len(languages)) + r\"\"\"}{c}{$QE(s_F,h_F) / QE(s_M,h_M)$ } \\\\\n",
    "    & \"\"\" + \" & \".join(languages) + r\"\"\" \\\\\n",
    "    \\midrule\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate table rows for each model\n",
    "    table_rows = \"\"\n",
    "    for model in models:\n",
    "        # Get data for the current model\n",
    "        model_data = df[df['model'] == model].set_index('lang')\n",
    "\n",
    "        # Construct the row with ratio first and significance in parentheses\n",
    "        row = model + \" & \" + \" & \".join(\n",
    "            [\n",
    "                f\"{model_data.loc[lang, 'ref_ratio_mean']:.3f} \" +\n",
    "                (r\"\\gabad\" if model_data.loc[lang, 'stat_significance'] else r\"\\gagood\") + \"\"\n",
    "                if lang in model_data.index else \"!\"\n",
    "                for lang in languages\n",
    "            ]\n",
    "        ) + r\" \\\\\" + \"\\n\"\n",
    "        \n",
    "        # Add spacing after certain models (optional)\n",
    "        if model == models[2] or model == models[4]:\n",
    "            row += r\"\\addlinespace[0.2cm]\" + \"\\n\"\n",
    "        \n",
    "        table_rows += row\n",
    "\n",
    "    # Define the LaTeX table footer\n",
    "    table_footer = r\"\"\"\n",
    "    \\bottomrule\n",
    "    \\end{tabular}\n",
    "    \\caption{\\textbf{MT-GenEval Counterfactual Results} on human-written translations. \n",
    "    A green check mark (\\cmark) indicates that correctly-inflected feminine references receive statistically significantly (p<0.05) lower scores than correctly-inflected masculine references. Numbers represent the ratio mean, and significance results are shown in parentheses.} \n",
    "    \\label{tab:contextual_stat_tests}\n",
    "    \\end{table*}\n",
    "    \"\"\"\n",
    "\n",
    "    # Combine header, rows, and footer\n",
    "    latex_table = table_header + table_rows + table_footer\n",
    "    \n",
    "    return latex_table\n",
    "\n",
    "\n",
    "\n",
    "ref_stat_sign_copy = results_df_cont[['model','lang','ref_significance_ratio','ref_ratio_mean']].copy()\n",
    "ref_stat_sign_copy = ref_stat_sign_copy.rename({'ref_significance_ratio':'stat_significance'},axis=1).copy()\n",
    "stat_table_latex = create_significance_table_QE_ratio(ref_stat_sign_copy)\n",
    "print(stat_table_latex)\n",
    "# print(stat_table_latex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 1 of paper QE ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ref_ratio_mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GPT 4</th>\n",
       "      <td>1.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gemma 2 9B</th>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kiwi 22</th>\n",
       "      <td>0.997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kiwi 23 XL</th>\n",
       "      <td>1.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kiwi 23 XXL</th>\n",
       "      <td>1.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Llama 3.1 70B</th>\n",
       "      <td>0.995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MetricX23 LARGE</th>\n",
       "      <td>0.998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MetricX23 XL</th>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mistral 7B</th>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xCOMET XL</th>\n",
       "      <td>0.988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xCOMET XXL</th>\n",
       "      <td>0.995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ref_ratio_mean\n",
       "model                          \n",
       "GPT 4                     1.001\n",
       "Gemma 2 9B                1.000\n",
       "Kiwi 22                   0.997\n",
       "Kiwi 23 XL                1.001\n",
       "Kiwi 23 XXL               1.003\n",
       "Llama 3.1 70B             0.995\n",
       "MetricX23 LARGE           0.998\n",
       "MetricX23 XL              1.000\n",
       "Mistral 7B                1.000\n",
       "xCOMET XL                 0.988\n",
       "xCOMET XXL                0.995"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df_cont_aggr = results_df_cont.pivot_table(index='model',values=['ref_ratio_mean'])\n",
    "results_df_cont_aggr.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tables for prediction-analysis\n",
    "Results are computed with analysis_mt_geneval_counterfactual_all_models.ipynb and shown below. \n",
    "\n",
    "- Total Error Rate  aggregated over languages\n",
    "- Ratio Φ = ER(S^F) / ER(S^M)  aggregated over languages\n",
    "- Statistical significance test of Φ  per language (bootstrap resampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_overall_analysis_path = './results-copied/stats/nonambiguous-counterfactual/references/prediction-analysis/data.csv'\n",
    "results_df = pd.read_csv(res_overall_analysis_path)\n",
    "errors_df = results_df[[\"model\",\"lang\",\"ref_error_rate_total\",\n",
    "                                      \"ref_error_rate_male\",\n",
    "                                      \"ref_error_rate_fem\",\n",
    "                                      \"ref_error_rate_ratio\",\n",
    "                                      \"ref_stat_significance\",\n",
    "                                      ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 1 of paper ER_R and Φ_R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ref_error_rate_ratio</th>\n",
       "      <th>ref_error_rate_total</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GPT 4</th>\n",
       "      <td>1.15</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gemma 2 9B</th>\n",
       "      <td>1.36</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kiwi 22</th>\n",
       "      <td>1.70</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kiwi 23 XL</th>\n",
       "      <td>1.18</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kiwi 23 XXL</th>\n",
       "      <td>0.87</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Llama 3.1 70B</th>\n",
       "      <td>1.16</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MetricX23 LARGE</th>\n",
       "      <td>1.25</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MetricX23 XL</th>\n",
       "      <td>1.19</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mistral 7B</th>\n",
       "      <td>1.13</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xCOMET XL</th>\n",
       "      <td>1.81</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xCOMET XXL</th>\n",
       "      <td>1.32</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ref_error_rate_ratio  ref_error_rate_total\n",
       "model                                                      \n",
       "GPT 4                            1.15                  0.16\n",
       "Gemma 2 9B                       1.36                  0.28\n",
       "Kiwi 22                          1.70                  0.11\n",
       "Kiwi 23 XL                       1.18                  0.09\n",
       "Kiwi 23 XXL                      0.87                  0.07\n",
       "Llama 3.1 70B                    1.16                  0.31\n",
       "MetricX23 LARGE                  1.25                  0.31\n",
       "MetricX23 XL                     1.19                  0.12\n",
       "Mistral 7B                       1.13                  0.74\n",
       "xCOMET XL                        1.81                  0.10\n",
       "xCOMET XXL                       1.32                  0.08"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors_df_aggr = errors_df.pivot_table(index='model',values=[\"ref_error_rate_total\",'ref_error_rate_ratio'])\n",
    "errors_df_aggr.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    \\begin{table*}\n",
      "    \\centering\n",
      "    \\small\n",
      "    \\begin{tabular}{lcccccccc}\n",
      "    \\toprule\n",
      "    \\rowcolor{white}\n",
      "    \\textbf{Metrics} & \\multicolumn{8}{c}{$\\Phi(s_F,s_M)$} \\\\\n",
      "    & de & es & fr & it & pt & ru & hi & ar \\\\\n",
      "    \\midrule\n",
      "    GPT 4 & 0.857 \\gagood & 1.200 \\gabad & 0.939 \\gagood & 1.167 \\gabad & 1.037 \\gabad & 1.368 \\gabad & 1.214 \\gabad & 1.400 \\gabad \\\\\n",
      "Gemma 2 9B & 0.775 \\gagood & 1.317 \\gabad & 1.767 \\gabad & 1.443 \\gabad & 1.459 \\gabad & 1.421 \\gabad & 1.248 \\gabad & 1.426 \\gabad \\\\\n",
      "Kiwi 22 & 2.500 \\gabad & 1.909 \\gabad & 2.333 \\gabad & 1.773 \\gabad & 0.938 \\gagood & 2.500 \\gabad & 0.683 \\gagood & 1.000 \\gagood \\\\\n",
      "\\addlinespace[0.2cm]\n",
      "Kiwi 23 XL & 1.143 \\gabad & 1.136 \\gabad & 0.500 \\gagood & 1.000 \\gagood & 1.333 \\gabad & 2.333 \\gabad & 1.297 \\gabad & 0.706 \\gagood \\\\\n",
      "Kiwi 23 XXL & 0.286 \\gagood & 0.923 \\gagood & 0.750 \\gagood & 1.036 \\gabad & 0.692 \\gagood & 2.000 \\gabad & 1.000 \\gagood & 0.273 \\gagood \\\\\n",
      "\\addlinespace[0.2cm]\n",
      "Llama 3.1 70B & 0.902 \\gagood & 1.265 \\gabad & 1.113 \\gabad & 1.161 \\gabad & 1.140 \\gabad & 1.185 \\gabad & 1.304 \\gabad & 1.210 \\gabad \\\\\n",
      "MetricX23 LARGE & 1.037 \\gabad & 1.582 \\gabad & 1.459 \\gabad & 1.186 \\gabad & 0.839 \\gagood & 1.302 \\gabad & 0.937 \\gagood & 1.660 \\gabad \\\\\n",
      "MetricX23 XL & 1.600 \\gabad & 1.083 \\gabad & 0.857 \\gagood & 0.959 \\gagood & 0.550 \\gagood & 2.182 \\gabad & 1.182 \\gabad & 1.130 \\gabad \\\\\n",
      "Mistral 7B & 1.190 \\gabad & 1.125 \\gabad & 1.184 \\gabad & 1.194 \\gabad & 1.098 \\gabad & 1.191 \\gabad & 1.045 \\gabad & 0.990 \\gagood \\\\\n",
      "xCOMET XL & 3.000 \\gabad & 1.154 \\gabad & 2.200 \\gabad & 1.346 \\gabad & 2.429 \\gabad & 2.200 \\gabad & 1.181 \\gabad & 1.000 \\gagood \\\\\n",
      "xCOMET XXL & 0.857 \\gagood & 1.333 \\gabad & 1.143 \\gabad & 1.321 \\gabad & 1.111 \\gabad & 2.500 \\gabad & 1.426 \\gabad & 0.833 \\gagood \\\\\n",
      "\n",
      "    \\bottomrule\n",
      "    \\end{tabular}\n",
      "    \\caption{\\textbf{MT-GenEval Counterfactual Results} on human-written translations. \n",
      "    A green check mark (\\cmark) indicates that metrics make statistically significantly (p<0.05) more errors on sources with feminine referents compared to their masculine counterparts. } \n",
      "    \\label{tab:contextual_stat_tests}\n",
      "    \\end{table*}\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "def create_significance_table_Phi_ratiov2(df):\n",
    "    # Extract unique models and languages from the DataFrame\n",
    "    models = df['model'].unique()\n",
    "    languages = ['de', 'es', 'fr', 'it', 'pt', 'ru', 'hi', 'ar']  # Custom language order\n",
    "\n",
    "    # Define the LaTeX table header\n",
    "    table_header = r\"\"\"\n",
    "    \\begin{table*}\n",
    "    \\centering\n",
    "    \\small\n",
    "    \\begin{tabular}{l\"\"\" + \"c\" * len(languages) + r\"\"\"}\n",
    "    \\toprule\n",
    "    \\rowcolor{white}\n",
    "    \\textbf{Metrics} & \\multicolumn{\"\"\" + str(len(languages)) + r\"\"\"}{c}{$\\Phi(s_F,s_M)$} \\\\\n",
    "    & \"\"\" + \" & \".join(languages) + r\"\"\" \\\\\n",
    "    \\midrule\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate table rows for each model\n",
    "    table_rows = \"\"\n",
    "    for model in models:\n",
    "        # Get data for the current model\n",
    "        model_data = df[df['model'] == model].set_index('lang')\n",
    "\n",
    "        # Construct the row with values and significance\n",
    "        row = model + \" & \" + \" & \".join(\n",
    "            [\n",
    "                f\"{model_data.loc[lang, 'ref_error_rate_ratio']:.3f} \" +\n",
    "                (r\"\\gabad\" if model_data.loc[lang, 'stat_significance'] else r\"\\gagood\") + \"\"\n",
    "                if lang in model_data.index else \"!\"\n",
    "                for lang in languages\n",
    "            ]\n",
    "        ) + r\" \\\\\" + \"\\n\"\n",
    "\n",
    "        # Add spacing after certain models (optional)\n",
    "        if model == models[2] or model == models[4]:\n",
    "            row += r\"\\addlinespace[0.2cm]\" + \"\\n\"\n",
    "\n",
    "        table_rows += row\n",
    "\n",
    "    # Define the LaTeX table footer\n",
    "    table_footer = r\"\"\"\n",
    "    \\bottomrule\n",
    "    \\end{tabular}\n",
    "    \\caption{\\textbf{MT-GenEval Counterfactual Results} on human-written translations. \n",
    "    A green check mark (\\cmark) indicates that metrics make statistically significantly (p<0.05) more errors on sources with feminine referents compared to their masculine counterparts. } \n",
    "    \\label{tab:contextual_stat_tests}\n",
    "    \\end{table*}\n",
    "    \"\"\"\n",
    "\n",
    "    # Combine header, rows, and footer\n",
    "    latex_table = table_header + table_rows + table_footer\n",
    "    \n",
    "    return latex_table\n",
    "\n",
    "\n",
    "\n",
    "ref_stat_sign_copy = errors_df[['model','lang','ref_stat_significance','ref_error_rate_ratio']].copy()\n",
    "ref_stat_sign_copy = ref_stat_sign_copy.rename({'ref_stat_significance':'stat_significance'},axis=1).copy()\n",
    "stat_table_latex = create_significance_table_Phi_ratiov2(ref_stat_sign_copy)\n",
    "print(stat_table_latex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of taking the average of ratios Φ over languages, I present here another method that is more robust to outliers. This basically aggregates error rates over languages ER(S^F) \n",
    "and ER(S^M) and the computes the ratio of averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_overall_analysis_path = os.path.join(Root_path,'stats/counterfactual/prediction-analysis/data.csv')\n",
    "# results_df = pd.read_csv(res_overall_analysis_path)\n",
    "# errors_df = results_df[[\"model\",\"lang\",\"ref_error_rate_total\",\n",
    "#                                       \"ref_error_rate_male\",\n",
    "#                                       \"ref_error_rate_fem\",\n",
    "#                                       ]]\n",
    "# errors_df_aggr = errors_df.pivot_table(index='model',values=[\"ref_error_rate_total\",\n",
    "#                                       \"ref_error_rate_male\",\n",
    "#                                       \"ref_error_rate_fem\"])\n",
    "# errors_df_aggr[\"ref_error_ratio\"] = errors_df_aggr[\"ref_error_rate_fem\"]/errors_df_aggr[\"ref_error_rate_male\"]\n",
    "# errors_df_aggr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counterfactual Automatic Translations Section (GT translations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    \\begin{table*}\n",
      "    \\centering\n",
      "    \\small\n",
      "    \\begin{tabular}{lcccccccc}\n",
      "    \\toprule\n",
      "    \\rowcolor{white}\n",
      "    \\textbf{Metrics} & \\multicolumn{8}{c}{$QE(s_F,h_F) / QE(s_M,h_M)$ } \\\\\n",
      "    & de & es & fr & it & pt & ru & hi & ar \\\\\n",
      "    \\midrule\n",
      "    Kiwi 22 & 0.998 \\gagood & 0.997 \\gabad & 0.996 \\gabad & 0.996 \\gabad & 0.998 \\gagood & 0.996 \\gagood & 0.997 \\gabad & 0.997 \\gagood \\\\\n",
      "Kiwi 23 XL & 0.999 \\gagood & 0.998 \\gagood & 0.996 \\gagood & 1.011 \\gagood & 1.004 \\gagood & 0.990 \\gabad & 1.006 \\gagood & 0.993 \\gagood \\\\\n",
      "Kiwi 23 XXL & 1.003 \\gagood & 0.996 \\gagood & 1.024 \\gagood & 1.000 \\gagood & 1.013 \\gagood & 0.994 \\gagood & 0.996 \\gagood & 0.996 \\gagood \\\\\n",
      "\\addlinespace[0.2cm]\n",
      "MetricX23 LARGE & 1.000 \\gagood & 1.000 \\gagood & 0.998 \\gabad & 0.998 \\gabad & 0.999 \\gagood & 0.999 \\gagood & 1.000 \\gagood & 0.992 \\gabad \\\\\n",
      "MetricX23 XL & 1.002 \\gagood & 1.001 \\gagood & 1.000 \\gagood & 0.999 \\gagood & 0.997 \\gagood & 1.004 \\gagood & 1.003 \\gagood & 1.001 \\gagood \\\\\n",
      "\\addlinespace[0.2cm]\n",
      "xCOMET XL & 0.993 \\gabad & 0.985 \\gabad & 0.985 \\gabad & 0.994 \\gagood & 0.993 \\gagood & 0.983 \\gabad & 0.981 \\gabad & 1.003 \\gagood \\\\\n",
      "xCOMET XXL & 0.999 \\gagood & 0.995 \\gagood & 0.994 \\gagood & 0.999 \\gagood & 0.999 \\gagood & 0.991 \\gagood & 1.005 \\gagood & 0.996 \\gagood \\\\\n",
      "\n",
      "    \\bottomrule\n",
      "    \\end{tabular}\n",
      "    \\caption{\\textbf{MT-GenEval Counterfactual Results} on human-written translations. \n",
      "    A green check mark (\\cmark) indicates that correctly-inflected feminine references receive statistically significantly (p<0.05) lower scores than correctly-inflected masculine references. Numbers represent the ratio mean, and significance results are shown in parentheses.} \n",
      "    \\label{tab:contextual_stat_tests}\n",
      "    \\end{table*}\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "res_overall_analysis_path = './results-copied/stats/nonambiguous-counterfactual/gt-translations/continuous-analysis/results.csv'\n",
    "results_df_cont = pd.read_csv(res_overall_analysis_path)\n",
    "\n",
    "\n",
    "def create_significance_table_QE_ratio(df):\n",
    "    # Extract unique models and languages from the DataFrame\n",
    "    models = df['model'].unique()\n",
    "    languages = ['de', 'es', 'fr', 'it', 'pt', 'ru', 'hi', 'ar']\n",
    "\n",
    "    # Define the LaTeX table header\n",
    "    table_header = r\"\"\"\n",
    "    \\begin{table*}\n",
    "    \\centering\n",
    "    \\small\n",
    "    \\begin{tabular}{l\"\"\" + \"c\" * len(languages) + r\"\"\"}\n",
    "    \\toprule\n",
    "    \\rowcolor{white}\n",
    "    \\textbf{Metrics} & \\multicolumn{\"\"\" + str(len(languages)) + r\"\"\"}{c}{$QE(s_F,h_F) / QE(s_M,h_M)$ } \\\\\n",
    "    & \"\"\" + \" & \".join(languages) + r\"\"\" \\\\\n",
    "    \\midrule\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate table rows for each model\n",
    "    table_rows = \"\"\n",
    "    for model in models:\n",
    "        # Get data for the current model\n",
    "        model_data = df[df['model'] == model].set_index('lang')\n",
    "\n",
    "        # Construct the row with ratio first and significance in parentheses\n",
    "        row = model + \" & \" + \" & \".join(\n",
    "            [\n",
    "                f\"{model_data.loc[lang, 'qe_ratio_mean']:.3f} \" +\n",
    "                (r\"\\gabad\" if model_data.loc[lang, 'stat_significance'] else r\"\\gagood\") + \"\"\n",
    "                if lang in model_data.index else \"!\"\n",
    "                for lang in languages\n",
    "            ]\n",
    "        ) + r\" \\\\\" + \"\\n\"\n",
    "        \n",
    "        # Add spacing after certain models (optional)\n",
    "        if model == models[2] or model == models[4]:\n",
    "            row += r\"\\addlinespace[0.2cm]\" + \"\\n\"\n",
    "        \n",
    "        table_rows += row\n",
    "\n",
    "    # Define the LaTeX table footer\n",
    "    table_footer = r\"\"\"\n",
    "    \\bottomrule\n",
    "    \\end{tabular}\n",
    "    \\caption{\\textbf{MT-GenEval Counterfactual Results} on human-written translations. \n",
    "    A green check mark (\\cmark) indicates that correctly-inflected feminine references receive statistically significantly (p<0.05) lower scores than correctly-inflected masculine references. Numbers represent the ratio mean, and significance results are shown in parentheses.} \n",
    "    \\label{tab:contextual_stat_tests}\n",
    "    \\end{table*}\n",
    "    \"\"\"\n",
    "\n",
    "    # Combine header, rows, and footer\n",
    "    latex_table = table_header + table_rows + table_footer\n",
    "    \n",
    "    return latex_table\n",
    "\n",
    "\n",
    "GT_stat_sign_copy = results_df_cont[['model','lang','gt_significance_ratio','gt_ratio_mean']].copy()\n",
    "GT_stat_sign_copy = GT_stat_sign_copy.rename({'gt_significance_ratio':'stat_significance','gt_ratio_mean':'qe_ratio_mean'},axis=1).copy()\n",
    "stat_table_latex = create_significance_table_QE_ratio(GT_stat_sign_copy)\n",
    "print(stat_table_latex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregated QE ratios for GT translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gt_ratio_mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Kiwi 22</th>\n",
       "      <td>0.997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kiwi 23 XL</th>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kiwi 23 XXL</th>\n",
       "      <td>1.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MetricX23 LARGE</th>\n",
       "      <td>0.999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MetricX23 XL</th>\n",
       "      <td>1.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xCOMET XL</th>\n",
       "      <td>0.990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xCOMET XXL</th>\n",
       "      <td>0.997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 gt_ratio_mean\n",
       "model                         \n",
       "Kiwi 22                  0.997\n",
       "Kiwi 23 XL               1.000\n",
       "Kiwi 23 XXL              1.003\n",
       "MetricX23 LARGE          0.999\n",
       "MetricX23 XL             1.001\n",
       "xCOMET XL                0.990\n",
       "xCOMET XXL               0.997"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df_cont_aggr = results_df_cont.pivot_table(index='model',values=['gt_ratio_mean'])\n",
    "results_df_cont_aggr.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table for prediction based analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_overall_analysis_path = './results-copied/stats/nonambiguous-counterfactual/gt-translations/prediction-analysis/results.csv'\n",
    "results_df = pd.read_csv(res_overall_analysis_path)\n",
    "errors_df = results_df[[\"model\",\"lang\",\"gt_error_rate_total\",\n",
    "                                      \"gt_error_rate_male\",\n",
    "                                      \"gt_error_rate_fem\",\n",
    "                                      \"gt_error_rate_ratio\",\n",
    "                                      \"gt_stat_significance\",\n",
    "                                      ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregated results over languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gt_error_rate_ratio</th>\n",
       "      <th>gt_error_rate_total</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Kiwi 22</th>\n",
       "      <td>1.34</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kiwi 23 XL</th>\n",
       "      <td>1.60</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kiwi 23 XXL</th>\n",
       "      <td>1.62</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MetricX23 LARGE</th>\n",
       "      <td>1.17</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MetricX23 XL</th>\n",
       "      <td>1.33</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xCOMET XL</th>\n",
       "      <td>1.96</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xCOMET XXL</th>\n",
       "      <td>1.23</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 gt_error_rate_ratio  gt_error_rate_total\n",
       "model                                                    \n",
       "Kiwi 22                         1.34                 0.13\n",
       "Kiwi 23 XL                      1.60                 0.11\n",
       "Kiwi 23 XXL                     1.62                 0.09\n",
       "MetricX23 LARGE                 1.17                 0.34\n",
       "MetricX23 XL                    1.33                 0.14\n",
       "xCOMET XL                       1.96                 0.12\n",
       "xCOMET XXL                      1.23                 0.10"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors_df_aggr = errors_df.pivot_table(index='model',values=[\"gt_error_rate_total\",'gt_error_rate_ratio'])\n",
    "errors_df_aggr.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    \\begin{table*}\n",
      "    \\centering\n",
      "    \\small\n",
      "    \\begin{tabular}{lcccccccc}\n",
      "    \\toprule\n",
      "    \\rowcolor{white}\n",
      "    \\textbf{Metrics} & \\multicolumn{8}{c}{$\\Phi(s_F,s_M)$} \\\\\n",
      "    & de & es & fr & it & pt & ru & hi & ar \\\\\n",
      "    \\midrule\n",
      "    Kiwi 22 & 0.571 \\gagood & 1.269 \\gabad & 2.000 \\gabad & 1.292 \\gabad & 1.100 \\gabad & 2.333 \\gabad & 0.526 \\gagood & 1.667 \\gabad \\\\\n",
      "Kiwi 23 XL & 1.600 \\gabad & 1.143 \\gabad & 0.833 \\gagood & 1.160 \\gabad & 1.000 \\gagood & 3.667 \\gabad & 1.053 \\gabad & 2.333 \\gabad \\\\\n",
      "Kiwi 23 XXL & 0.600 \\gagood & 1.125 \\gabad & 4.000 \\gabad & 1.000 \\gagood & 0.667 \\gagood & 1.500 \\gabad & 1.030 \\gabad & 3.000 \\gabad \\\\\n",
      "\\addlinespace[0.2cm]\n",
      "MetricX23 LARGE & 1.086 \\gabad & 1.216 \\gabad & 1.115 \\gabad & 1.208 \\gabad & 0.973 \\gagood & 1.290 \\gabad & 0.966 \\gagood & 1.474 \\gabad \\\\\n",
      "MetricX23 XL & 1.429 \\gabad & 1.207 \\gabad & 1.286 \\gabad & 1.125 \\gabad & 1.400 \\gabad & 2.143 \\gabad & 1.071 \\gabad & 1.000 \\gagood \\\\\n",
      "\\addlinespace[0.2cm]\n",
      "xCOMET XL & 2.000 \\gabad & 1.238 \\gabad & 4.000 \\gabad & 1.304 \\gabad & 2.571 \\gabad & 1.800 \\gabad & 0.958 \\gagood & 1.778 \\gabad \\\\\n",
      "xCOMET XXL & 0.667 \\gagood & 1.286 \\gabad & 1.750 \\gabad & 1.038 \\gabad & 1.429 \\gabad & 1.000 \\gagood & 1.167 \\gabad & 1.500 \\gabad \\\\\n",
      "\n",
      "    \\bottomrule\n",
      "    \\end{tabular}\n",
      "    \\caption{\\textbf{MT-GenEval Counterfactual Results} on human-written translations. \n",
      "    A green check mark (\\cmark) indicates that metrics make statistically significantly (p<0.05) more errors on sources with feminine referents compared to their masculine counterparts. } \n",
      "    \\label{tab:contextual_stat_tests}\n",
      "    \\end{table*}\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "def create_significance_table_Phi_ratiov2(df):\n",
    "    # Extract unique models and languages from the DataFrame\n",
    "    models = df['model'].unique()\n",
    "    languages = ['de', 'es', 'fr', 'it', 'pt', 'ru', 'hi', 'ar']  # Custom language order\n",
    "\n",
    "    # Define the LaTeX table header\n",
    "    table_header = r\"\"\"\n",
    "    \\begin{table*}\n",
    "    \\centering\n",
    "    \\small\n",
    "    \\begin{tabular}{l\"\"\" + \"c\" * len(languages) + r\"\"\"}\n",
    "    \\toprule\n",
    "    \\rowcolor{white}\n",
    "    \\textbf{Metrics} & \\multicolumn{\"\"\" + str(len(languages)) + r\"\"\"}{c}{$\\Phi(s_F,s_M)$} \\\\\n",
    "    & \"\"\" + \" & \".join(languages) + r\"\"\" \\\\\n",
    "    \\midrule\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate table rows for each model\n",
    "    table_rows = \"\"\n",
    "    for model in models:\n",
    "        # Get data for the current model\n",
    "        model_data = df[df['model'] == model].set_index('lang')\n",
    "\n",
    "        # Construct the row with values and significance\n",
    "        row = model + \" & \" + \" & \".join(\n",
    "            [\n",
    "                f\"{model_data.loc[lang, 'error_rate_ratio']:.3f} \" +\n",
    "                (r\"\\gabad\" if model_data.loc[lang, 'stat_significance'] else r\"\\gagood\") + \"\"\n",
    "                if lang in model_data.index else \"!\"\n",
    "                for lang in languages\n",
    "            ]\n",
    "        ) + r\" \\\\\" + \"\\n\"\n",
    "\n",
    "        # Add spacing after certain models (optional)\n",
    "        if model == models[2] or model == models[4]:\n",
    "            row += r\"\\addlinespace[0.2cm]\" + \"\\n\"\n",
    "\n",
    "        table_rows += row\n",
    "\n",
    "    # Define the LaTeX table footer\n",
    "    table_footer = r\"\"\"\n",
    "    \\bottomrule\n",
    "    \\end{tabular}\n",
    "    \\caption{\\textbf{MT-GenEval Counterfactual Results} on human-written translations. \n",
    "    A green check mark (\\cmark) indicates that metrics make statistically significantly (p<0.05) more errors on sources with feminine referents compared to their masculine counterparts. } \n",
    "    \\label{tab:contextual_stat_tests}\n",
    "    \\end{table*}\n",
    "    \"\"\"\n",
    "\n",
    "    # Combine header, rows, and footer\n",
    "    latex_table = table_header + table_rows + table_footer\n",
    "    \n",
    "    return latex_table\n",
    "\n",
    "\n",
    "\n",
    "gt_stat_sign_copy = errors_df[['model','lang','gt_stat_significance','gt_error_rate_ratio']].copy()\n",
    "gt_stat_sign_copy = gt_stat_sign_copy.rename({'gt_stat_significance':'stat_significance','gt_error_rate_ratio':'error_rate_ratio'},axis=1).copy()\n",
    "stat_table_latex = create_significance_table_Phi_ratiov2(gt_stat_sign_copy)\n",
    "print(stat_table_latex)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
