{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4f3ebad-c5be-46d8-8199-b7ea35044f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a1f7373-6ddd-4e24-886d-3cc4b3724809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.chdir(\"../\")\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# from notebooks.utils import gini\n",
    "from scipy.stats import mannwhitneyu, pearsonr, wilcoxon\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "from src import eval_metrics\n",
    "\n",
    "sns.set_theme(\"paper\", style=\"whitegrid\")\n",
    "plt.rcParams[\"axes.grid\"] = True\n",
    "plt.rcParams[\"grid.color\"] = \"lightgray\"\n",
    "plt.rcParams[\"grid.linestyle\"] = \"--\"\n",
    "plt.rcParams[\"grid.linewidth\"] = 0.5\n",
    "\n",
    "plt.rc(\"text\", usetex=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8780e254-3157-4668-b7d2-9f585a39904a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ffba6274-2eea-4294-a741-72f952f91446",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"MTGenEval\"\n",
    "MODELS = {\n",
    "\n",
    "    \"Kiwi 22\": {\n",
    "        \"model_id\": \"Unbabel--wmt22-cometkiwi-da\",\n",
    "        \"type\": \"neural_metric\"\n",
    "    },\n",
    "    \"Kiwi 23 XL\": {\n",
    "        \"model_id\": \"Unbabel--wmt23-cometkiwi-da-xl\",\n",
    "        \"type\": \"neural_metric\"\n",
    "    },\n",
    "    \"Kiwi 23 XXL\": {\n",
    "        \"model_id\": \"Unbabel--wmt23-cometkiwi-da-xxl\",\n",
    "        \"type\": \"neural_metric\"\n",
    "    },\n",
    "    \"xCOMET XL\": {\n",
    "        \"model_id\": \"Unbabel--XCOMET-XL\",\n",
    "        \"type\": \"neural_metric\"\n",
    "    },\n",
    "    \"xCOMET XXL\": {\n",
    "        \"model_id\": \"Unbabel--XCOMET-XXL\",\n",
    "        \"type\": \"neural_metric\"\n",
    "    },\n",
    "    \"MetricX23 LARGE\": {\n",
    "        \"model_id\": \"google--metricx-23-qe-large-v2p0\",\n",
    "        \"type\": \"neural_metric\"\n",
    "    },\n",
    "    \"MetricX23 XL\": {\n",
    "        \"model_id\": \"google--metricx-23-qe-xl-v2p0\",\n",
    "        \"type\": \"neural_metric\"\n",
    "    },\n",
    "\n",
    "    \"Llama 3.1 70B\": {\n",
    "        \"model_id\": \"meta-llama--Meta-Llama-3.1-70B-Instruct\",\n",
    "        \"type\": \"LLM\"\n",
    "    },\n",
    "    \n",
    "    \"Mistral 7B\": {\n",
    "        \"model_id\": \"mistralai--Mistral-7B-Instruct-v0.2\",\n",
    "        \"type\": \"LLM\"\n",
    "    },\n",
    "\n",
    "  \n",
    "    \"Gemma 2 9B\": {\n",
    "        \"model_id\": \"google--gemma-2-9b-it\",\n",
    "        \"type\": \"LLM\"\n",
    "    },\n",
    "\n",
    "    \"GPT 4\": {\n",
    "        \"model_id\": \"gpt-4o-2024-05-13\",\n",
    "        \"type\": \"LLM\"\n",
    "    }\n",
    "}\n",
    "           \n",
    "LANGS = [\"it\",\"es\",\"de\",\"pt\",\"ar\",\"fr\",\"hi\",\"ru\"]\n",
    "\n",
    "tot = list()\n",
    "\n",
    "#define the following cases according to how context is used in hypothesis:\n",
    "# - with-original-contexts -- the source context is used (prepended) in the hypothesis\n",
    "# - with-translated-contexts -- the source context translated into the target lang is used (prepended) in the hypothesis\n",
    "metrics_context_options = {\"with-original-contexts\":\"original ctx\",\"with-translated-contexts\":\"translated ctx\"}\n",
    "translation_model_dict = {\"facebook--nllb-200-3.3B\":\"NLLB 3.3B\",\"google-translate\":\"GT\" }\n",
    "llms_ctx_options = {\"standard\":\"standard ctx\",\"paraphrased\":\"paraphrased ctx\"}\n",
    "\n",
    "for lang in LANGS:\n",
    "    # open source gender info file\n",
    "    gender_info = pd.read_csv(f\"./data/mtgeneval/context_genders/geneval-context-en{lang}-genders.txt\", sep=\"\\t\")\n",
    "    for m, info in MODELS.items():\n",
    "        if info[\"type\"]== \"neural_metric\":\n",
    "            for ctx_v,ctx_n in metrics_context_options.items():\n",
    "                if ctx_v==\"with-translated-contexts\":\n",
    "                    for trans_v,trans_m in translation_model_dict.items():\n",
    "                        res = pd.read_csv(f\"./results-copied/scores/nonambiguous-contextual/mtgeneval/{lang}/{info['model_id']}/{ctx_v}/{trans_v}/scores.csv\")\n",
    "                        res[\"model\"] = m+\"--\"+ctx_n +\"--\"+trans_m\n",
    "                        res[\"lang\"] = lang\n",
    "                        res[\"src_gender\"] = gender_info[\"gender\"]\n",
    "                        res[\"type\"] = info[\"type\"]\n",
    "                        tot.append(res)\n",
    "                else:\n",
    "                    res = pd.read_csv(f\"./results-copied/scores/nonambiguous-contextual/mtgeneval/{lang}/{info['model_id']}/{ctx_v}/scores.csv\")\n",
    "                    res[\"model\"] = m+\"--\"+ctx_n \n",
    "                    res[\"lang\"] = lang\n",
    "                    res[\"src_gender\"] = gender_info[\"gender\"]\n",
    "                    res[\"type\"] = info[\"type\"]\n",
    "                    tot.append(res)\n",
    "        elif info[\"type\"]==\"LLM\":\n",
    "            for ctx_v,ctx_n in llms_ctx_options.items():\n",
    "                if m==\"GPT 4\" and ctx_n==\"paraphrased ctx\":\n",
    "                    continue # we skip this version (as it is not done for GPT4)\n",
    "                res = pd.read_csv(f\"./results-copied/scores/nonambiguous-contextual/mtgeneval/{lang}/{info['model_id']}/{ctx_v}/scores.csv\")\n",
    "                res[\"model\"] = m +\"--\" + ctx_n\n",
    "                res[\"lang\"] = lang\n",
    "                res[\"src_gender\"] = gender_info[\"gender\"]\n",
    "                res[\"type\"] = info[\"type\"]\n",
    "                tot.append(res)\n",
    "        else:\n",
    "            assert False,\"Type error!\"\n",
    "res = pd.concat(tot)\n",
    "data_all = res\n",
    "data_all = data_all.rename(columns={\"score_Original\":\"score_original\", \"score_Flipped\":\"score_flipped\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51be20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: ar  # 1100  samples before taking intersection of models. \n",
      "Language: ar kept # 904  samples (after intersection between all models). \n",
      "Language: de  # 1100  samples before taking intersection of models. \n",
      "Language: de kept # 913  samples (after intersection between all models). \n",
      "Language: es  # 1096  samples before taking intersection of models. \n",
      "Language: es kept # 1002  samples (after intersection between all models). \n",
      "Language: fr  # 1099  samples before taking intersection of models. \n",
      "Language: fr kept # 968  samples (after intersection between all models). \n",
      "Language: hi  # 1098  samples before taking intersection of models. \n",
      "Language: hi kept # 969  samples (after intersection between all models). \n",
      "Language: it  # 1094  samples before taking intersection of models. \n",
      "Language: it kept # 954  samples (after intersection between all models). \n",
      "Language: pt  # 1089  samples before taking intersection of models. \n",
      "Language: pt kept # 985  samples (after intersection between all models). \n",
      "Language: ru  # 1100  samples before taking intersection of models. \n",
      "Language: ru kept # 965  samples (after intersection between all models). \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>context</th>\n",
       "      <th>source</th>\n",
       "      <th>reference_original</th>\n",
       "      <th>reference_flipped</th>\n",
       "      <th>score_original</th>\n",
       "      <th>score_flipped</th>\n",
       "      <th>model</th>\n",
       "      <th>lang</th>\n",
       "      <th>src_gender</th>\n",
       "      <th>type</th>\n",
       "      <th>score_M</th>\n",
       "      <th>score_F</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Slap-guitarist Miyavi, who has his roots in vi...</td>\n",
       "      <td>It was the most successful international tour ...</td>\n",
       "      <td>كانت من أنجح الرحلات الدولية التي نفذها فنان ي...</td>\n",
       "      <td>كانت من أنجح الرحلات الدولية التي نفذتها فنانة...</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.70</td>\n",
       "      <td>GPT 4--standard ctx</td>\n",
       "      <td>ar</td>\n",
       "      <td>male</td>\n",
       "      <td>LLM</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Whittier was heavily influenced by the doctrin...</td>\n",
       "      <td>Whittier was first introduced to poetry by a t...</td>\n",
       "      <td>تعرّف ويتري على الشعر لأول مرة عن طريق معلم.</td>\n",
       "      <td>تعرّفت ويتري على الشعر لأول مرة عن طريق معلم.</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.85</td>\n",
       "      <td>GPT 4--standard ctx</td>\n",
       "      <td>ar</td>\n",
       "      <td>male</td>\n",
       "      <td>LLM</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>He was later appointed as a Civil, Commercial ...</td>\n",
       "      <td>Guinle entered politics as Government Secretar...</td>\n",
       "      <td>دخل Guinle السياسة كسكرتير المحافظة لبلدية Com...</td>\n",
       "      <td>دخلت Guinle السياسة كسكرتير المحافظة لبلدية Co...</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.60</td>\n",
       "      <td>GPT 4--standard ctx</td>\n",
       "      <td>ar</td>\n",
       "      <td>male</td>\n",
       "      <td>LLM</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                            context  \\\n",
       "2   2  Slap-guitarist Miyavi, who has his roots in vi...   \n",
       "3   3  Whittier was heavily influenced by the doctrin...   \n",
       "5   5  He was later appointed as a Civil, Commercial ...   \n",
       "\n",
       "                                              source  \\\n",
       "2  It was the most successful international tour ...   \n",
       "3  Whittier was first introduced to poetry by a t...   \n",
       "5  Guinle entered politics as Government Secretar...   \n",
       "\n",
       "                                  reference_original  \\\n",
       "2  كانت من أنجح الرحلات الدولية التي نفذها فنان ي...   \n",
       "3       تعرّف ويتري على الشعر لأول مرة عن طريق معلم.   \n",
       "5  دخل Guinle السياسة كسكرتير المحافظة لبلدية Com...   \n",
       "\n",
       "                                   reference_flipped  score_original  \\\n",
       "2  كانت من أنجح الرحلات الدولية التي نفذتها فنانة...            0.95   \n",
       "3      تعرّفت ويتري على الشعر لأول مرة عن طريق معلم.            0.95   \n",
       "5  دخلت Guinle السياسة كسكرتير المحافظة لبلدية Co...            0.75   \n",
       "\n",
       "   score_flipped                model lang src_gender type  score_M  score_F  \n",
       "2           0.70  GPT 4--standard ctx   ar       male  LLM     0.95     0.70  \n",
       "3           0.85  GPT 4--standard ctx   ar       male  LLM     0.95     0.85  \n",
       "5           0.60  GPT 4--standard ctx   ar       male  LLM     0.75     0.60  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "final_df = list()\n",
    "original_shapes = {}\n",
    "\n",
    "for (lang, model), subdf in data_all.groupby([\"lang\", \"model\"]):\n",
    "    \n",
    "    original_shape = subdf.shape\n",
    "    # print(lang,model ,original_shape)\n",
    "    if lang not in original_shapes:\n",
    "        original_shapes[lang]=original_shape\n",
    "        \n",
    "    # print(lang, model, original_shape)\n",
    "    filtered = subdf.copy()\n",
    "    filtered = filtered.dropna(subset=[\"score_original\", \"score_flipped\"])\n",
    "    # rescale scores of LLMs from 0-100 to 0-1\n",
    "    if filtered.iloc[0][\"type\"] == \"LLM\":\n",
    "        filtered[\"score_original\"] = filtered[\"score_original\"] / 100\n",
    "        filtered[\"score_flipped\"] = filtered[\"score_flipped\"] / 100\n",
    "    \n",
    "        # filter out all rows that have scores outside the range [0, 1]\n",
    "        filtered = filtered.loc[\n",
    "            (filtered[\"score_original\"] > 0.1) & \\\n",
    "            (filtered[\"score_original\"] <= 1) & \\\n",
    "            (filtered[\"score_flipped\"] > 0.1) & \\\n",
    "            (filtered[\"score_flipped\"] <= 1)\n",
    "        ]\n",
    "    elif \"MetricX\" in filtered.iloc[0][\"model\"]:\n",
    "        filtered[\"score_original\"] = 1 - filtered[\"score_original\"] / 25\n",
    "        filtered[\"score_flipped\"] = 1 - filtered[\"score_flipped\"] / 25\n",
    "    else:\n",
    "        #no filtering needed for classical neural metrics!\n",
    "        pass\n",
    "    final_df.append(filtered)\n",
    "\n",
    "data_all = pd.concat(final_df).copy()\n",
    "\n",
    "\n",
    "# Compute the intersection of rows to be kept for each language\n",
    "keep_rows_inter = {}\n",
    "for lang, lang_df in data_all.groupby(\"lang\"):\n",
    "    common_rows = set(lang_df.index)\n",
    "    for model, model_df in lang_df.groupby(\"model\"):\n",
    "        common_rows.intersection_update(set(model_df.index))\n",
    "    keep_rows_inter[lang] = list(common_rows)\n",
    "\n",
    "\n",
    "for lang in keep_rows_inter:\n",
    "    print(f\"Language: {lang}  # {original_shapes[lang][0]}  samples before taking intersection of models. \")\n",
    "    print(f\"Language: {lang} kept # {len(keep_rows_inter[lang])}  samples (after intersection between all models). \")\n",
    "\n",
    "# then we filter again!\n",
    "final_df=[]\n",
    "removed_info={lang:{} for lang in LANGS}\n",
    "\n",
    "for (lang, model), subdf in data_all.groupby([\"lang\", \"model\"]):\n",
    "    filtered=subdf.copy()\n",
    "    filtered = filtered.loc[keep_rows_inter[lang]]\n",
    "    removed_info[lang][model] =  100 * (1 - filtered.shape[0] / original_shapes[lang][0])\n",
    "\n",
    "    final_df.append(filtered)\n",
    "data_all = pd.concat(final_df).copy()\n",
    "data_all[\"score_M\"] = data_all.apply(lambda x: x[\"score_original\"] if x[\"src_gender\"] == \"male\" else x[\"score_flipped\"], axis=1)\n",
    "data_all[\"score_F\"] = data_all.apply(lambda x: x[\"score_original\"] if x[\"src_gender\"] == \"female\" else x[\"score_flipped\"], axis=1)\n",
    "\n",
    "#we remove the GT based models (ones that use google translate for translating the context).This is done only for this part of analysis\n",
    "data_all = data_all[~data_all['model'].str.contains('GT', case=True, na=False)]\n",
    "data_all.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "133a2e6e-818b-495a-8ea8-c15160d19aca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>it</th>\n",
       "      <th>es</th>\n",
       "      <th>de</th>\n",
       "      <th>pt</th>\n",
       "      <th>ar</th>\n",
       "      <th>fr</th>\n",
       "      <th>hi</th>\n",
       "      <th>ru</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GPT 4--standard ctx</th>\n",
       "      <td>12.797075</td>\n",
       "      <td>8.576642</td>\n",
       "      <td>17.0</td>\n",
       "      <td>9.550046</td>\n",
       "      <td>17.818182</td>\n",
       "      <td>11.919927</td>\n",
       "      <td>11.748634</td>\n",
       "      <td>12.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gemma 2 9B--paraphrased ctx</th>\n",
       "      <td>12.797075</td>\n",
       "      <td>8.576642</td>\n",
       "      <td>17.0</td>\n",
       "      <td>9.550046</td>\n",
       "      <td>17.818182</td>\n",
       "      <td>11.919927</td>\n",
       "      <td>11.748634</td>\n",
       "      <td>12.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gemma 2 9B--standard ctx</th>\n",
       "      <td>12.797075</td>\n",
       "      <td>8.576642</td>\n",
       "      <td>17.0</td>\n",
       "      <td>9.550046</td>\n",
       "      <td>17.818182</td>\n",
       "      <td>11.919927</td>\n",
       "      <td>11.748634</td>\n",
       "      <td>12.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kiwi 22--original ctx</th>\n",
       "      <td>12.797075</td>\n",
       "      <td>8.576642</td>\n",
       "      <td>17.0</td>\n",
       "      <td>9.550046</td>\n",
       "      <td>17.818182</td>\n",
       "      <td>11.919927</td>\n",
       "      <td>11.748634</td>\n",
       "      <td>12.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kiwi 22--translated ctx--GT</th>\n",
       "      <td>12.797075</td>\n",
       "      <td>8.576642</td>\n",
       "      <td>17.0</td>\n",
       "      <td>9.550046</td>\n",
       "      <td>17.818182</td>\n",
       "      <td>11.919927</td>\n",
       "      <td>11.748634</td>\n",
       "      <td>12.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kiwi 22--translated ctx--NLLB 3.3B</th>\n",
       "      <td>12.797075</td>\n",
       "      <td>8.576642</td>\n",
       "      <td>17.0</td>\n",
       "      <td>9.550046</td>\n",
       "      <td>17.818182</td>\n",
       "      <td>11.919927</td>\n",
       "      <td>11.748634</td>\n",
       "      <td>12.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kiwi 23 XL--original ctx</th>\n",
       "      <td>12.797075</td>\n",
       "      <td>8.576642</td>\n",
       "      <td>17.0</td>\n",
       "      <td>9.550046</td>\n",
       "      <td>17.818182</td>\n",
       "      <td>11.919927</td>\n",
       "      <td>11.748634</td>\n",
       "      <td>12.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kiwi 23 XL--translated ctx--GT</th>\n",
       "      <td>12.797075</td>\n",
       "      <td>8.576642</td>\n",
       "      <td>17.0</td>\n",
       "      <td>9.550046</td>\n",
       "      <td>17.818182</td>\n",
       "      <td>11.919927</td>\n",
       "      <td>11.748634</td>\n",
       "      <td>12.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kiwi 23 XL--translated ctx--NLLB 3.3B</th>\n",
       "      <td>12.797075</td>\n",
       "      <td>8.576642</td>\n",
       "      <td>17.0</td>\n",
       "      <td>9.550046</td>\n",
       "      <td>17.818182</td>\n",
       "      <td>11.919927</td>\n",
       "      <td>11.748634</td>\n",
       "      <td>12.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kiwi 23 XXL--original ctx</th>\n",
       "      <td>12.797075</td>\n",
       "      <td>8.576642</td>\n",
       "      <td>17.0</td>\n",
       "      <td>9.550046</td>\n",
       "      <td>17.818182</td>\n",
       "      <td>11.919927</td>\n",
       "      <td>11.748634</td>\n",
       "      <td>12.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kiwi 23 XXL--translated ctx--GT</th>\n",
       "      <td>12.797075</td>\n",
       "      <td>8.576642</td>\n",
       "      <td>17.0</td>\n",
       "      <td>9.550046</td>\n",
       "      <td>17.818182</td>\n",
       "      <td>11.919927</td>\n",
       "      <td>11.748634</td>\n",
       "      <td>12.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kiwi 23 XXL--translated ctx--NLLB 3.3B</th>\n",
       "      <td>12.797075</td>\n",
       "      <td>8.576642</td>\n",
       "      <td>17.0</td>\n",
       "      <td>9.550046</td>\n",
       "      <td>17.818182</td>\n",
       "      <td>11.919927</td>\n",
       "      <td>11.748634</td>\n",
       "      <td>12.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Llama 3.1 70B--paraphrased ctx</th>\n",
       "      <td>12.797075</td>\n",
       "      <td>8.576642</td>\n",
       "      <td>17.0</td>\n",
       "      <td>9.550046</td>\n",
       "      <td>17.818182</td>\n",
       "      <td>11.919927</td>\n",
       "      <td>11.748634</td>\n",
       "      <td>12.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Llama 3.1 70B--standard ctx</th>\n",
       "      <td>12.797075</td>\n",
       "      <td>8.576642</td>\n",
       "      <td>17.0</td>\n",
       "      <td>9.550046</td>\n",
       "      <td>17.818182</td>\n",
       "      <td>11.919927</td>\n",
       "      <td>11.748634</td>\n",
       "      <td>12.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MetricX23 LARGE--original ctx</th>\n",
       "      <td>12.797075</td>\n",
       "      <td>8.576642</td>\n",
       "      <td>17.0</td>\n",
       "      <td>9.550046</td>\n",
       "      <td>17.818182</td>\n",
       "      <td>11.919927</td>\n",
       "      <td>11.748634</td>\n",
       "      <td>12.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MetricX23 LARGE--translated ctx--GT</th>\n",
       "      <td>12.797075</td>\n",
       "      <td>8.576642</td>\n",
       "      <td>17.0</td>\n",
       "      <td>9.550046</td>\n",
       "      <td>17.818182</td>\n",
       "      <td>11.919927</td>\n",
       "      <td>11.748634</td>\n",
       "      <td>12.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MetricX23 LARGE--translated ctx--NLLB 3.3B</th>\n",
       "      <td>12.797075</td>\n",
       "      <td>8.576642</td>\n",
       "      <td>17.0</td>\n",
       "      <td>9.550046</td>\n",
       "      <td>17.818182</td>\n",
       "      <td>11.919927</td>\n",
       "      <td>11.748634</td>\n",
       "      <td>12.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MetricX23 XL--original ctx</th>\n",
       "      <td>12.797075</td>\n",
       "      <td>8.576642</td>\n",
       "      <td>17.0</td>\n",
       "      <td>9.550046</td>\n",
       "      <td>17.818182</td>\n",
       "      <td>11.919927</td>\n",
       "      <td>11.748634</td>\n",
       "      <td>12.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MetricX23 XL--translated ctx--GT</th>\n",
       "      <td>12.797075</td>\n",
       "      <td>8.576642</td>\n",
       "      <td>17.0</td>\n",
       "      <td>9.550046</td>\n",
       "      <td>17.818182</td>\n",
       "      <td>11.919927</td>\n",
       "      <td>11.748634</td>\n",
       "      <td>12.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MetricX23 XL--translated ctx--NLLB 3.3B</th>\n",
       "      <td>12.797075</td>\n",
       "      <td>8.576642</td>\n",
       "      <td>17.0</td>\n",
       "      <td>9.550046</td>\n",
       "      <td>17.818182</td>\n",
       "      <td>11.919927</td>\n",
       "      <td>11.748634</td>\n",
       "      <td>12.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mistral 7B--paraphrased ctx</th>\n",
       "      <td>12.797075</td>\n",
       "      <td>8.576642</td>\n",
       "      <td>17.0</td>\n",
       "      <td>9.550046</td>\n",
       "      <td>17.818182</td>\n",
       "      <td>11.919927</td>\n",
       "      <td>11.748634</td>\n",
       "      <td>12.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mistral 7B--standard ctx</th>\n",
       "      <td>12.797075</td>\n",
       "      <td>8.576642</td>\n",
       "      <td>17.0</td>\n",
       "      <td>9.550046</td>\n",
       "      <td>17.818182</td>\n",
       "      <td>11.919927</td>\n",
       "      <td>11.748634</td>\n",
       "      <td>12.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xCOMET XL--original ctx</th>\n",
       "      <td>12.797075</td>\n",
       "      <td>8.576642</td>\n",
       "      <td>17.0</td>\n",
       "      <td>9.550046</td>\n",
       "      <td>17.818182</td>\n",
       "      <td>11.919927</td>\n",
       "      <td>11.748634</td>\n",
       "      <td>12.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xCOMET XL--translated ctx--GT</th>\n",
       "      <td>12.797075</td>\n",
       "      <td>8.576642</td>\n",
       "      <td>17.0</td>\n",
       "      <td>9.550046</td>\n",
       "      <td>17.818182</td>\n",
       "      <td>11.919927</td>\n",
       "      <td>11.748634</td>\n",
       "      <td>12.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xCOMET XL--translated ctx--NLLB 3.3B</th>\n",
       "      <td>12.797075</td>\n",
       "      <td>8.576642</td>\n",
       "      <td>17.0</td>\n",
       "      <td>9.550046</td>\n",
       "      <td>17.818182</td>\n",
       "      <td>11.919927</td>\n",
       "      <td>11.748634</td>\n",
       "      <td>12.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xCOMET XXL--original ctx</th>\n",
       "      <td>12.797075</td>\n",
       "      <td>8.576642</td>\n",
       "      <td>17.0</td>\n",
       "      <td>9.550046</td>\n",
       "      <td>17.818182</td>\n",
       "      <td>11.919927</td>\n",
       "      <td>11.748634</td>\n",
       "      <td>12.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xCOMET XXL--translated ctx--GT</th>\n",
       "      <td>12.797075</td>\n",
       "      <td>8.576642</td>\n",
       "      <td>17.0</td>\n",
       "      <td>9.550046</td>\n",
       "      <td>17.818182</td>\n",
       "      <td>11.919927</td>\n",
       "      <td>11.748634</td>\n",
       "      <td>12.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xCOMET XXL--translated ctx--NLLB 3.3B</th>\n",
       "      <td>12.797075</td>\n",
       "      <td>8.576642</td>\n",
       "      <td>17.0</td>\n",
       "      <td>9.550046</td>\n",
       "      <td>17.818182</td>\n",
       "      <td>11.919927</td>\n",
       "      <td>11.748634</td>\n",
       "      <td>12.272727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   it        es    de  \\\n",
       "GPT 4--standard ctx                         12.797075  8.576642  17.0   \n",
       "Gemma 2 9B--paraphrased ctx                 12.797075  8.576642  17.0   \n",
       "Gemma 2 9B--standard ctx                    12.797075  8.576642  17.0   \n",
       "Kiwi 22--original ctx                       12.797075  8.576642  17.0   \n",
       "Kiwi 22--translated ctx--GT                 12.797075  8.576642  17.0   \n",
       "Kiwi 22--translated ctx--NLLB 3.3B          12.797075  8.576642  17.0   \n",
       "Kiwi 23 XL--original ctx                    12.797075  8.576642  17.0   \n",
       "Kiwi 23 XL--translated ctx--GT              12.797075  8.576642  17.0   \n",
       "Kiwi 23 XL--translated ctx--NLLB 3.3B       12.797075  8.576642  17.0   \n",
       "Kiwi 23 XXL--original ctx                   12.797075  8.576642  17.0   \n",
       "Kiwi 23 XXL--translated ctx--GT             12.797075  8.576642  17.0   \n",
       "Kiwi 23 XXL--translated ctx--NLLB 3.3B      12.797075  8.576642  17.0   \n",
       "Llama 3.1 70B--paraphrased ctx              12.797075  8.576642  17.0   \n",
       "Llama 3.1 70B--standard ctx                 12.797075  8.576642  17.0   \n",
       "MetricX23 LARGE--original ctx               12.797075  8.576642  17.0   \n",
       "MetricX23 LARGE--translated ctx--GT         12.797075  8.576642  17.0   \n",
       "MetricX23 LARGE--translated ctx--NLLB 3.3B  12.797075  8.576642  17.0   \n",
       "MetricX23 XL--original ctx                  12.797075  8.576642  17.0   \n",
       "MetricX23 XL--translated ctx--GT            12.797075  8.576642  17.0   \n",
       "MetricX23 XL--translated ctx--NLLB 3.3B     12.797075  8.576642  17.0   \n",
       "Mistral 7B--paraphrased ctx                 12.797075  8.576642  17.0   \n",
       "Mistral 7B--standard ctx                    12.797075  8.576642  17.0   \n",
       "xCOMET XL--original ctx                     12.797075  8.576642  17.0   \n",
       "xCOMET XL--translated ctx--GT               12.797075  8.576642  17.0   \n",
       "xCOMET XL--translated ctx--NLLB 3.3B        12.797075  8.576642  17.0   \n",
       "xCOMET XXL--original ctx                    12.797075  8.576642  17.0   \n",
       "xCOMET XXL--translated ctx--GT              12.797075  8.576642  17.0   \n",
       "xCOMET XXL--translated ctx--NLLB 3.3B       12.797075  8.576642  17.0   \n",
       "\n",
       "                                                  pt         ar         fr  \\\n",
       "GPT 4--standard ctx                         9.550046  17.818182  11.919927   \n",
       "Gemma 2 9B--paraphrased ctx                 9.550046  17.818182  11.919927   \n",
       "Gemma 2 9B--standard ctx                    9.550046  17.818182  11.919927   \n",
       "Kiwi 22--original ctx                       9.550046  17.818182  11.919927   \n",
       "Kiwi 22--translated ctx--GT                 9.550046  17.818182  11.919927   \n",
       "Kiwi 22--translated ctx--NLLB 3.3B          9.550046  17.818182  11.919927   \n",
       "Kiwi 23 XL--original ctx                    9.550046  17.818182  11.919927   \n",
       "Kiwi 23 XL--translated ctx--GT              9.550046  17.818182  11.919927   \n",
       "Kiwi 23 XL--translated ctx--NLLB 3.3B       9.550046  17.818182  11.919927   \n",
       "Kiwi 23 XXL--original ctx                   9.550046  17.818182  11.919927   \n",
       "Kiwi 23 XXL--translated ctx--GT             9.550046  17.818182  11.919927   \n",
       "Kiwi 23 XXL--translated ctx--NLLB 3.3B      9.550046  17.818182  11.919927   \n",
       "Llama 3.1 70B--paraphrased ctx              9.550046  17.818182  11.919927   \n",
       "Llama 3.1 70B--standard ctx                 9.550046  17.818182  11.919927   \n",
       "MetricX23 LARGE--original ctx               9.550046  17.818182  11.919927   \n",
       "MetricX23 LARGE--translated ctx--GT         9.550046  17.818182  11.919927   \n",
       "MetricX23 LARGE--translated ctx--NLLB 3.3B  9.550046  17.818182  11.919927   \n",
       "MetricX23 XL--original ctx                  9.550046  17.818182  11.919927   \n",
       "MetricX23 XL--translated ctx--GT            9.550046  17.818182  11.919927   \n",
       "MetricX23 XL--translated ctx--NLLB 3.3B     9.550046  17.818182  11.919927   \n",
       "Mistral 7B--paraphrased ctx                 9.550046  17.818182  11.919927   \n",
       "Mistral 7B--standard ctx                    9.550046  17.818182  11.919927   \n",
       "xCOMET XL--original ctx                     9.550046  17.818182  11.919927   \n",
       "xCOMET XL--translated ctx--GT               9.550046  17.818182  11.919927   \n",
       "xCOMET XL--translated ctx--NLLB 3.3B        9.550046  17.818182  11.919927   \n",
       "xCOMET XXL--original ctx                    9.550046  17.818182  11.919927   \n",
       "xCOMET XXL--translated ctx--GT              9.550046  17.818182  11.919927   \n",
       "xCOMET XXL--translated ctx--NLLB 3.3B       9.550046  17.818182  11.919927   \n",
       "\n",
       "                                                   hi         ru  \n",
       "GPT 4--standard ctx                         11.748634  12.272727  \n",
       "Gemma 2 9B--paraphrased ctx                 11.748634  12.272727  \n",
       "Gemma 2 9B--standard ctx                    11.748634  12.272727  \n",
       "Kiwi 22--original ctx                       11.748634  12.272727  \n",
       "Kiwi 22--translated ctx--GT                 11.748634  12.272727  \n",
       "Kiwi 22--translated ctx--NLLB 3.3B          11.748634  12.272727  \n",
       "Kiwi 23 XL--original ctx                    11.748634  12.272727  \n",
       "Kiwi 23 XL--translated ctx--GT              11.748634  12.272727  \n",
       "Kiwi 23 XL--translated ctx--NLLB 3.3B       11.748634  12.272727  \n",
       "Kiwi 23 XXL--original ctx                   11.748634  12.272727  \n",
       "Kiwi 23 XXL--translated ctx--GT             11.748634  12.272727  \n",
       "Kiwi 23 XXL--translated ctx--NLLB 3.3B      11.748634  12.272727  \n",
       "Llama 3.1 70B--paraphrased ctx              11.748634  12.272727  \n",
       "Llama 3.1 70B--standard ctx                 11.748634  12.272727  \n",
       "MetricX23 LARGE--original ctx               11.748634  12.272727  \n",
       "MetricX23 LARGE--translated ctx--GT         11.748634  12.272727  \n",
       "MetricX23 LARGE--translated ctx--NLLB 3.3B  11.748634  12.272727  \n",
       "MetricX23 XL--original ctx                  11.748634  12.272727  \n",
       "MetricX23 XL--translated ctx--GT            11.748634  12.272727  \n",
       "MetricX23 XL--translated ctx--NLLB 3.3B     11.748634  12.272727  \n",
       "Mistral 7B--paraphrased ctx                 11.748634  12.272727  \n",
       "Mistral 7B--standard ctx                    11.748634  12.272727  \n",
       "xCOMET XL--original ctx                     11.748634  12.272727  \n",
       "xCOMET XL--translated ctx--GT               11.748634  12.272727  \n",
       "xCOMET XL--translated ctx--NLLB 3.3B        11.748634  12.272727  \n",
       "xCOMET XXL--original ctx                    11.748634  12.272727  \n",
       "xCOMET XXL--translated ctx--GT              11.748634  12.272727  \n",
       "xCOMET XXL--translated ctx--NLLB 3.3B       11.748634  12.272727  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "removed_info = pd.DataFrame(removed_info)\n",
    "removed_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f908ef9d-bdee-4a4f-952a-fd27a55a4fc0",
   "metadata": {},
   "source": [
    "Probability Analysis - Now we compute the continuous metrics along with their statistical significance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7ae90205-2576-4fb3-ae66-e95565cb302d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/home/manos/.local/lib/python3.10/site-packages/pandas/core/nanops.py:1016: RuntimeWarning: invalid value encountered in subtract\n",
      "  sqr = _ensure_numeric((avg - values) ** 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "stats_list = list()\n",
    "for (model, lang), subdf in data_all.groupby([\"model\",\"lang\"]):\n",
    "\n",
    "    n_male = (subdf[\"src_gender\"]==\"male\").sum()\n",
    "    n_female = (subdf[\"src_gender\"]==\"female\").sum()\n",
    "\n",
    "    ratios,p_val_1samp_ratio,ratio_one_sample_test_less = eval_metrics.Ratio({\"F\": subdf[\"score_F\"], \"M\": subdf[\"score_M\"]}).score()\n",
    "    d = dict(\n",
    "        model=model,\n",
    "        lang=lang,\n",
    "        score_M_mean=subdf[\"score_M\"].mean(),\n",
    "        score_M_std=subdf[\"score_M\"].std(),\n",
    "        score_F_mean=subdf[\"score_F\"].mean(),\n",
    "        score_F_std=subdf[\"score_F\"].std(),\n",
    "        ratio_mean = ratios.mean(),\n",
    "        ratio_std = ratios.std(),\n",
    "        #1sample test results -- for ratio\n",
    "        p_val_1samp_ratio = p_val_1samp_ratio,\n",
    "        sign_1samp_ratio = ratio_one_sample_test_less,\n",
    "    )\n",
    "\n",
    "    stats_list.append(d)\n",
    "\n",
    "stats_df = pd.DataFrame(stats_list).reset_index()\n",
    "stats_df = stats_df.drop('index',axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e812b7a-518c-4836-ac7a-b8caad070259",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df_pivot = stats_df.pivot_table(index=[\"model\"],columns=[\"lang\"],\n",
    "                              values=[\"score_M_mean\",\n",
    "                                      \"score_M_std\",\n",
    "                                      \"score_F_mean\",\n",
    "                                      \"score_F_std\",\n",
    "                                      \"ratio_mean\",\n",
    "                                      \"ratio_std\",\n",
    "                                      \"p_val_1samp_ratio\",\n",
    "                                      \"sign_1samp_ratio\"\n",
    "                                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ee3dd1e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>lang</th>\n",
       "      <th>score_M_mean</th>\n",
       "      <th>score_M_std</th>\n",
       "      <th>score_F_mean</th>\n",
       "      <th>score_F_std</th>\n",
       "      <th>ratio_mean</th>\n",
       "      <th>ratio_std</th>\n",
       "      <th>p_val_1samp_ratio</th>\n",
       "      <th>sign_1samp_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>MetricX23 LARGE--translated ctx--NLLB 3.3B</td>\n",
       "      <td>ar</td>\n",
       "      <td>0.389957</td>\n",
       "      <td>0.213707</td>\n",
       "      <td>0.387581</td>\n",
       "      <td>0.211442</td>\n",
       "      <td>inf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>MetricX23 XL--translated ctx--NLLB 3.3B</td>\n",
       "      <td>ar</td>\n",
       "      <td>0.053370</td>\n",
       "      <td>0.176147</td>\n",
       "      <td>0.055079</td>\n",
       "      <td>0.179070</td>\n",
       "      <td>inf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>MetricX23 XL--translated ctx--NLLB 3.3B</td>\n",
       "      <td>es</td>\n",
       "      <td>0.923873</td>\n",
       "      <td>0.069902</td>\n",
       "      <td>0.919592</td>\n",
       "      <td>0.071475</td>\n",
       "      <td>0.995855</td>\n",
       "      <td>0.033781</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          model lang  score_M_mean  \\\n",
       "96   MetricX23 LARGE--translated ctx--NLLB 3.3B   ar      0.389957   \n",
       "112     MetricX23 XL--translated ctx--NLLB 3.3B   ar      0.053370   \n",
       "114     MetricX23 XL--translated ctx--NLLB 3.3B   es      0.923873   \n",
       "\n",
       "     score_M_std  score_F_mean  score_F_std  ratio_mean  ratio_std  \\\n",
       "96      0.213707      0.387581     0.211442         inf        NaN   \n",
       "112     0.176147      0.055079     0.179070         inf        NaN   \n",
       "114     0.069902      0.919592     0.071475    0.995855   0.033781   \n",
       "\n",
       "     p_val_1samp_ratio  sign_1samp_ratio  \n",
       "96                 NaN             False  \n",
       "112                NaN             False  \n",
       "114                NaN             False  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for None values:\n",
    "stats_df[stats_df.isnull().any(axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "348a993e-0fd1-448b-9209-61c0b841ccd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('./results-copied/stats/nonambiguous-contextual/probability-analysis', exist_ok=True)\n",
    "stats_path = './results-copied/stats/nonambiguous-contextual/probability-analysis/results.csv'\n",
    "stats_df.to_csv(stats_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6864e65-174a-48b4-abad-0c8e3aecc979",
   "metadata": {},
   "source": [
    "#### (Prediction-view) Hard binary selection analysis \n",
    "We compute the following for each model and language:\n",
    "- Total error rate ER\n",
    "- Error Rate for feminine and masculine sources; ER(S^F), ER(S^M)\n",
    "- Error Rate Ratio Φ\n",
    "- Statistical significance of Φ using bootstrap resampling\n",
    "\n",
    "For the aggregated results across languages and the tables of the paper check make_figs_contextual_v2.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a84d1a68-b8fa-424f-bac1-26856cd3b711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT 4--standard ctx ar\n",
      "GPT 4--standard ctx de\n",
      "GPT 4--standard ctx es\n",
      "GPT 4--standard ctx fr\n",
      "GPT 4--standard ctx hi\n",
      "GPT 4--standard ctx it\n",
      "GPT 4--standard ctx pt\n",
      "GPT 4--standard ctx ru\n",
      "Gemma 2 9B--paraphrased ctx ar\n",
      "Gemma 2 9B--paraphrased ctx de\n",
      "Gemma 2 9B--paraphrased ctx es\n",
      "Gemma 2 9B--paraphrased ctx fr\n",
      "Gemma 2 9B--paraphrased ctx hi\n",
      "Gemma 2 9B--paraphrased ctx it\n",
      "Gemma 2 9B--paraphrased ctx pt\n",
      "Gemma 2 9B--paraphrased ctx ru\n",
      "Gemma 2 9B--standard ctx ar\n",
      "Gemma 2 9B--standard ctx de\n",
      "Gemma 2 9B--standard ctx es\n",
      "Gemma 2 9B--standard ctx fr\n",
      "Gemma 2 9B--standard ctx hi\n",
      "Gemma 2 9B--standard ctx it\n",
      "Gemma 2 9B--standard ctx pt\n",
      "Gemma 2 9B--standard ctx ru\n",
      "Kiwi 22--original ctx ar\n",
      "Kiwi 22--original ctx de\n",
      "Kiwi 22--original ctx es\n",
      "Kiwi 22--original ctx fr\n",
      "Kiwi 22--original ctx hi\n",
      "Kiwi 22--original ctx it\n",
      "Kiwi 22--original ctx pt\n",
      "Kiwi 22--original ctx ru\n",
      "Kiwi 22--translated ctx--NLLB 3.3B ar\n",
      "Kiwi 22--translated ctx--NLLB 3.3B de\n",
      "Kiwi 22--translated ctx--NLLB 3.3B es\n",
      "Kiwi 22--translated ctx--NLLB 3.3B fr\n",
      "Kiwi 22--translated ctx--NLLB 3.3B hi\n",
      "Kiwi 22--translated ctx--NLLB 3.3B it\n",
      "Kiwi 22--translated ctx--NLLB 3.3B pt\n",
      "Kiwi 22--translated ctx--NLLB 3.3B ru\n",
      "Kiwi 23 XL--original ctx ar\n",
      "Kiwi 23 XL--original ctx de\n",
      "Kiwi 23 XL--original ctx es\n",
      "Kiwi 23 XL--original ctx fr\n",
      "Kiwi 23 XL--original ctx hi\n",
      "Kiwi 23 XL--original ctx it\n",
      "Kiwi 23 XL--original ctx pt\n",
      "Kiwi 23 XL--original ctx ru\n",
      "Kiwi 23 XL--translated ctx--NLLB 3.3B ar\n",
      "Kiwi 23 XL--translated ctx--NLLB 3.3B de\n",
      "Kiwi 23 XL--translated ctx--NLLB 3.3B es\n",
      "Kiwi 23 XL--translated ctx--NLLB 3.3B fr\n",
      "Kiwi 23 XL--translated ctx--NLLB 3.3B hi\n",
      "Kiwi 23 XL--translated ctx--NLLB 3.3B it\n",
      "Kiwi 23 XL--translated ctx--NLLB 3.3B pt\n",
      "Kiwi 23 XL--translated ctx--NLLB 3.3B ru\n",
      "Kiwi 23 XXL--original ctx ar\n",
      "Kiwi 23 XXL--original ctx de\n",
      "Kiwi 23 XXL--original ctx es\n",
      "Kiwi 23 XXL--original ctx fr\n",
      "Kiwi 23 XXL--original ctx hi\n",
      "Kiwi 23 XXL--original ctx it\n",
      "Kiwi 23 XXL--original ctx pt\n",
      "Kiwi 23 XXL--original ctx ru\n",
      "Kiwi 23 XXL--translated ctx--NLLB 3.3B ar\n",
      "Kiwi 23 XXL--translated ctx--NLLB 3.3B de\n",
      "Kiwi 23 XXL--translated ctx--NLLB 3.3B es\n",
      "Kiwi 23 XXL--translated ctx--NLLB 3.3B fr\n",
      "Kiwi 23 XXL--translated ctx--NLLB 3.3B hi\n",
      "Kiwi 23 XXL--translated ctx--NLLB 3.3B it\n",
      "Kiwi 23 XXL--translated ctx--NLLB 3.3B pt\n",
      "Kiwi 23 XXL--translated ctx--NLLB 3.3B ru\n",
      "Llama 3.1 70B--paraphrased ctx ar\n",
      "Llama 3.1 70B--paraphrased ctx de\n",
      "Llama 3.1 70B--paraphrased ctx es\n",
      "Llama 3.1 70B--paraphrased ctx fr\n",
      "Llama 3.1 70B--paraphrased ctx hi\n",
      "Llama 3.1 70B--paraphrased ctx it\n",
      "Llama 3.1 70B--paraphrased ctx pt\n",
      "Llama 3.1 70B--paraphrased ctx ru\n",
      "Llama 3.1 70B--standard ctx ar\n",
      "Llama 3.1 70B--standard ctx de\n",
      "Llama 3.1 70B--standard ctx es\n",
      "Llama 3.1 70B--standard ctx fr\n",
      "Llama 3.1 70B--standard ctx hi\n",
      "Llama 3.1 70B--standard ctx it\n",
      "Llama 3.1 70B--standard ctx pt\n",
      "Llama 3.1 70B--standard ctx ru\n",
      "MetricX23 LARGE--original ctx ar\n",
      "MetricX23 LARGE--original ctx de\n",
      "MetricX23 LARGE--original ctx es\n",
      "MetricX23 LARGE--original ctx fr\n",
      "MetricX23 LARGE--original ctx hi\n",
      "MetricX23 LARGE--original ctx it\n",
      "MetricX23 LARGE--original ctx pt\n",
      "MetricX23 LARGE--original ctx ru\n",
      "MetricX23 LARGE--translated ctx--NLLB 3.3B ar\n",
      "MetricX23 LARGE--translated ctx--NLLB 3.3B de\n",
      "MetricX23 LARGE--translated ctx--NLLB 3.3B es\n",
      "MetricX23 LARGE--translated ctx--NLLB 3.3B fr\n",
      "MetricX23 LARGE--translated ctx--NLLB 3.3B hi\n",
      "MetricX23 LARGE--translated ctx--NLLB 3.3B it\n",
      "MetricX23 LARGE--translated ctx--NLLB 3.3B pt\n",
      "MetricX23 LARGE--translated ctx--NLLB 3.3B ru\n",
      "MetricX23 XL--original ctx ar\n",
      "MetricX23 XL--original ctx de\n",
      "MetricX23 XL--original ctx es\n",
      "MetricX23 XL--original ctx fr\n",
      "MetricX23 XL--original ctx hi\n",
      "MetricX23 XL--original ctx it\n",
      "MetricX23 XL--original ctx pt\n",
      "MetricX23 XL--original ctx ru\n",
      "MetricX23 XL--translated ctx--NLLB 3.3B ar\n",
      "MetricX23 XL--translated ctx--NLLB 3.3B de\n",
      "MetricX23 XL--translated ctx--NLLB 3.3B es\n",
      "MetricX23 XL--translated ctx--NLLB 3.3B fr\n",
      "MetricX23 XL--translated ctx--NLLB 3.3B hi\n",
      "MetricX23 XL--translated ctx--NLLB 3.3B it\n",
      "MetricX23 XL--translated ctx--NLLB 3.3B pt\n",
      "MetricX23 XL--translated ctx--NLLB 3.3B ru\n",
      "Mistral 7B--paraphrased ctx ar\n",
      "Mistral 7B--paraphrased ctx de\n",
      "Mistral 7B--paraphrased ctx es\n",
      "Mistral 7B--paraphrased ctx fr\n",
      "Mistral 7B--paraphrased ctx hi\n",
      "Mistral 7B--paraphrased ctx it\n",
      "Mistral 7B--paraphrased ctx pt\n",
      "Mistral 7B--paraphrased ctx ru\n",
      "Mistral 7B--standard ctx ar\n",
      "Mistral 7B--standard ctx de\n",
      "Mistral 7B--standard ctx es\n",
      "Mistral 7B--standard ctx fr\n",
      "Mistral 7B--standard ctx hi\n",
      "Mistral 7B--standard ctx it\n",
      "Mistral 7B--standard ctx pt\n",
      "Mistral 7B--standard ctx ru\n",
      "xCOMET XL--original ctx ar\n",
      "xCOMET XL--original ctx de\n",
      "xCOMET XL--original ctx es\n",
      "xCOMET XL--original ctx fr\n",
      "xCOMET XL--original ctx hi\n",
      "xCOMET XL--original ctx it\n",
      "xCOMET XL--original ctx pt\n",
      "xCOMET XL--original ctx ru\n",
      "xCOMET XL--translated ctx--NLLB 3.3B ar\n",
      "xCOMET XL--translated ctx--NLLB 3.3B de\n",
      "xCOMET XL--translated ctx--NLLB 3.3B es\n",
      "xCOMET XL--translated ctx--NLLB 3.3B fr\n",
      "xCOMET XL--translated ctx--NLLB 3.3B hi\n",
      "xCOMET XL--translated ctx--NLLB 3.3B it\n",
      "xCOMET XL--translated ctx--NLLB 3.3B pt\n",
      "xCOMET XL--translated ctx--NLLB 3.3B ru\n",
      "xCOMET XXL--original ctx ar\n",
      "xCOMET XXL--original ctx de\n",
      "xCOMET XXL--original ctx es\n",
      "xCOMET XXL--original ctx fr\n",
      "xCOMET XXL--original ctx hi\n",
      "xCOMET XXL--original ctx it\n",
      "xCOMET XXL--original ctx pt\n",
      "xCOMET XXL--original ctx ru\n",
      "xCOMET XXL--translated ctx--NLLB 3.3B ar\n",
      "xCOMET XXL--translated ctx--NLLB 3.3B de\n",
      "xCOMET XXL--translated ctx--NLLB 3.3B es\n",
      "xCOMET XXL--translated ctx--NLLB 3.3B fr\n",
      "xCOMET XXL--translated ctx--NLLB 3.3B hi\n",
      "xCOMET XXL--translated ctx--NLLB 3.3B it\n",
      "xCOMET XXL--translated ctx--NLLB 3.3B pt\n",
      "xCOMET XXL--translated ctx--NLLB 3.3B ru\n"
     ]
    }
   ],
   "source": [
    "\n",
    "acc_list = list()\n",
    "for (model, lang), subdf in data_all.groupby([\"model\",\"lang\"]):\n",
    "    print(model,lang)\n",
    "    acc = eval_metrics.Accuracy({\"F\": np.array(subdf[\"score_F\"]), \"M\": np.array(subdf[\"score_M\"]),\"y_true\":np.array(subdf[\"src_gender\"])}).score()\n",
    "    group_metrics = eval_metrics.GroupMetricsBootstraping({\"F\": np.array(subdf[\"score_F\"]), \"M\": np.array(subdf[\"score_M\"]),\"y_true\":np.array(subdf[\"src_gender\"])},alternative=\"greater\").stat_significance_with_paired_bootstrap()\n",
    "\n",
    "    d = dict(\n",
    "        model=model,\n",
    "        lang=lang,\n",
    "        acc_total = acc,\n",
    "        error_rate_total = group_metrics[\"results\"][\"total_error_rate\"],\n",
    "        error_rate_male = group_metrics[\"results\"][\"male\"][\"fnr\"],\n",
    "        error_rate_fem = group_metrics[\"results\"][\"female\"][\"fnr\"],\n",
    "        error_rate_diff =    group_metrics[\"results\"][\"fnr_diff\"],\n",
    "        error_rate_ratio =  group_metrics[\"results\"][\"fnr_ratio\"],\n",
    "        stat_significance = group_metrics[\"stat_significance\"],\n",
    "        p_value = group_metrics[\"p_value\"],\n",
    "    )\n",
    "\n",
    "    acc_list.append(d)\n",
    "\n",
    "acc_df = pd.DataFrame(acc_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "322b35c2-388b-4d50-a9f3-19708e661307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model                False\n",
       "lang                 False\n",
       "acc_total            False\n",
       "error_rate_total     False\n",
       "error_rate_male      False\n",
       "error_rate_fem       False\n",
       "error_rate_diff      False\n",
       "error_rate_ratio     False\n",
       "stat_significance    False\n",
       "p_value              False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "57cc80f4-bded-4f31-8251-c27bb4712626",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_df_pivot = acc_df.pivot_table(index=[\"model\"],columns=[\"lang\"],\n",
    "                              values=[\"acc_total\",\n",
    "                                      \"error_rate_total\",\n",
    "                                      \"error_rate_male\",\n",
    "                                      \"error_rate_fem\",\n",
    "                                      \"error_rate_diff\",\n",
    "                                      \"error_rate_ratio\",\n",
    "                                      \"stat_significance\",\n",
    "                                      \"p_value\"\n",
    "                                     ])\n",
    "os.makedirs('./results-copied/stats/nonambiguous-contextual/prediction-analysis', exist_ok=True)\n",
    "acc_df_path = './results-copied/stats/nonambiguous-contextual/prediction-analysis/results.csv'\n",
    "acc_df.to_csv(acc_df_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030b2005",
   "metadata": {},
   "source": [
    "### Measure ties\n",
    "Measure ties for the contextual non-ambiguous case. Used to create Table 10 of the paper. For table creation check (make_figs_contextual_v2.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4d7ff2d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>model</th>\n",
       "      <th>type</th>\n",
       "      <th>ties</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ar</td>\n",
       "      <td>GPT 4--standard ctx</td>\n",
       "      <td>LLM</td>\n",
       "      <td>44.800885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ar</td>\n",
       "      <td>Gemma 2 9B--paraphrased ctx</td>\n",
       "      <td>LLM</td>\n",
       "      <td>55.420354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ar</td>\n",
       "      <td>Gemma 2 9B--standard ctx</td>\n",
       "      <td>LLM</td>\n",
       "      <td>53.429204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ar</td>\n",
       "      <td>Kiwi 22--original ctx</td>\n",
       "      <td>neural_metric</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ar</td>\n",
       "      <td>Kiwi 22--translated ctx--NLLB 3.3B</td>\n",
       "      <td>neural_metric</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>ru</td>\n",
       "      <td>Mistral 7B--standard ctx</td>\n",
       "      <td>LLM</td>\n",
       "      <td>85.492228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>ru</td>\n",
       "      <td>xCOMET XL--original ctx</td>\n",
       "      <td>neural_metric</td>\n",
       "      <td>0.207254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>ru</td>\n",
       "      <td>xCOMET XL--translated ctx--NLLB 3.3B</td>\n",
       "      <td>neural_metric</td>\n",
       "      <td>0.103627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>ru</td>\n",
       "      <td>xCOMET XXL--original ctx</td>\n",
       "      <td>neural_metric</td>\n",
       "      <td>0.518135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>ru</td>\n",
       "      <td>xCOMET XXL--translated ctx--NLLB 3.3B</td>\n",
       "      <td>neural_metric</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>168 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    lang                                  model           type       ties\n",
       "0     ar                    GPT 4--standard ctx            LLM  44.800885\n",
       "1     ar            Gemma 2 9B--paraphrased ctx            LLM  55.420354\n",
       "2     ar               Gemma 2 9B--standard ctx            LLM  53.429204\n",
       "3     ar                  Kiwi 22--original ctx  neural_metric   0.000000\n",
       "4     ar     Kiwi 22--translated ctx--NLLB 3.3B  neural_metric   0.000000\n",
       "..   ...                                    ...            ...        ...\n",
       "163   ru               Mistral 7B--standard ctx            LLM  85.492228\n",
       "164   ru                xCOMET XL--original ctx  neural_metric   0.207254\n",
       "165   ru   xCOMET XL--translated ctx--NLLB 3.3B  neural_metric   0.103627\n",
       "166   ru               xCOMET XXL--original ctx  neural_metric   0.518135\n",
       "167   ru  xCOMET XXL--translated ctx--NLLB 3.3B  neural_metric   0.000000\n",
       "\n",
       "[168 rows x 4 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ties = []\n",
    "for (lang, model), subdf in data_all.groupby([\"lang\", \"model\"]):\n",
    "    ties.append({\"lang\": lang, \"model\": model,\"type\":subdf.iloc[0][\"type\"],\"ties\": 100*(subdf[\"score_M\"]==subdf[\"score_F\"]).sum()/len(subdf)})\n",
    "ties = pd.DataFrame(ties)\n",
    "ties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7be40796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('./results-copied/stats/nonambiguous-contextual/ties', exist_ok=True)\n",
    "ties_path_non_amb = './results-copied/stats/nonambiguous-contextual/ties/results.csv'\n",
    "ties.to_csv(ties_path_non_amb, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70daedd",
   "metadata": {},
   "source": [
    "#### Comparison Between BIL models:\n",
    "This creates the results for making Figure 9 of the paper. Comparing models with translated context by NLLB and GT. (for figure check make_figs_contextual_v2.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "db496048",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = {\n",
    "\n",
    "    \"Kiwi 22\": {\n",
    "        \"model_id\": \"Unbabel--wmt22-cometkiwi-da\",\n",
    "        \"type\": \"neural_metric\"\n",
    "    },\n",
    "    \"Kiwi 23 XL\": {\n",
    "        \"model_id\": \"Unbabel--wmt23-cometkiwi-da-xl\",\n",
    "        \"type\": \"neural_metric\"\n",
    "    },\n",
    "    \"Kiwi 23 XXL\": {\n",
    "        \"model_id\": \"Unbabel--wmt23-cometkiwi-da-xxl\",\n",
    "        \"type\": \"neural_metric\"\n",
    "    },\n",
    "    \"xCOMET XL\": {\n",
    "        \"model_id\": \"Unbabel--XCOMET-XL\",\n",
    "        \"type\": \"neural_metric\"\n",
    "    },\n",
    "    \"xCOMET XXL\": {\n",
    "        \"model_id\": \"Unbabel--XCOMET-XXL\",\n",
    "        \"type\": \"neural_metric\"\n",
    "    },\n",
    "    \"MetricX23 LARGE\": {\n",
    "        \"model_id\": \"google--metricx-23-qe-large-v2p0\",\n",
    "        \"type\": \"neural_metric\"\n",
    "    },\n",
    "    \"MetricX23 XL\": {\n",
    "        \"model_id\": \"google--metricx-23-qe-xl-v2p0\",\n",
    "        \"type\": \"neural_metric\"\n",
    "    },\n",
    "\n",
    "    \"Llama 3.1 70B\": {\n",
    "        \"model_id\": \"meta-llama--Meta-Llama-3.1-70B-Instruct\",\n",
    "        \"type\": \"LLM\"\n",
    "    },\n",
    "    \n",
    "    \"Mistral 7B\": {\n",
    "        \"model_id\": \"mistralai--Mistral-7B-Instruct-v0.2\",\n",
    "        \"type\": \"LLM\"\n",
    "    },\n",
    "\n",
    "  \n",
    "    \"Gemma 2 9B\": {\n",
    "        \"model_id\": \"google--gemma-2-9b-it\",\n",
    "        \"type\": \"LLM\"\n",
    "    },\n",
    "\n",
    "    \"GPT 4\": {\n",
    "        \"model_id\": \"gpt-4o-2024-05-13\",\n",
    "        \"type\": \"LLM\"\n",
    "    }\n",
    "}\n",
    "           \n",
    "LANGS = [\"it\",\"es\",\"de\",\"pt\",\"ar\",\"fr\",\"hi\",\"ru\"]\n",
    "\n",
    "tot = list()\n",
    "\n",
    "#define the following cases according to how context is used in hypothesis:\n",
    "# - with-original-contexts -- the source context is used (prepended) in the hypothesis\n",
    "# - with-translated-contexts -- the source context translated into the target lang is used (prepended) in the hypothesis\n",
    "metrics_context_options = {\"with-original-contexts\":\"original ctx\",\"with-translated-contexts\":\"translated ctx\"}\n",
    "translation_model_dict = {\"facebook--nllb-200-3.3B\":\"NLLB 3.3B\",\"google-translate\":\"GT\" }\n",
    "llms_ctx_options = {\"standard\":\"standard ctx\",\"paraphrased\":\"paraphrased ctx\"}\n",
    "\n",
    "for lang in LANGS:\n",
    "    # open source gender info file\n",
    "    gender_info = pd.read_csv(f\"./data/mtgeneval/context_genders/geneval-context-en{lang}-genders.txt\", sep=\"\\t\")\n",
    "    for m, info in MODELS.items():\n",
    "        if info[\"type\"]== \"neural_metric\":\n",
    "            for ctx_v,ctx_n in metrics_context_options.items():\n",
    "                if ctx_v==\"with-translated-contexts\":\n",
    "                    for trans_v,trans_m in translation_model_dict.items():\n",
    "                        res = pd.read_csv(f\"./results-copied/scores/nonambiguous-contextual/mtgeneval/{lang}/{info['model_id']}/{ctx_v}/{trans_v}/scores.csv\")\n",
    "                        res[\"model\"] = m+\"--\"+ctx_n +\"--\"+trans_m\n",
    "                        res[\"lang\"] = lang\n",
    "                        res[\"src_gender\"] = gender_info[\"gender\"]\n",
    "                        res[\"type\"] = info[\"type\"]\n",
    "                        tot.append(res)\n",
    "                else:\n",
    "                    res = pd.read_csv(f\"./results-copied/scores/nonambiguous-contextual/mtgeneval/{lang}/{info['model_id']}/{ctx_v}/scores.csv\")\n",
    "                    res[\"model\"] = m+\"--\"+ctx_n \n",
    "                    res[\"lang\"] = lang\n",
    "                    res[\"src_gender\"] = gender_info[\"gender\"]\n",
    "                    res[\"type\"] = info[\"type\"]\n",
    "                    tot.append(res)\n",
    "        elif info[\"type\"]==\"LLM\":\n",
    "            for ctx_v,ctx_n in llms_ctx_options.items():\n",
    "                if m==\"GPT 4\" and ctx_n==\"paraphrased ctx\":\n",
    "                    continue # we skip this version (as it is not done for GPT4)\n",
    "                res = pd.read_csv(f\"./results-copied/scores/nonambiguous-contextual/mtgeneval/{lang}/{info['model_id']}/{ctx_v}/scores.csv\")\n",
    "                res[\"model\"] = m +\"--\" + ctx_n\n",
    "                res[\"lang\"] = lang\n",
    "                res[\"src_gender\"] = gender_info[\"gender\"]\n",
    "                res[\"type\"] = info[\"type\"]\n",
    "                tot.append(res)\n",
    "        else:\n",
    "            assert False,\"Type error!\"\n",
    "res = pd.concat(tot)\n",
    "data_all = res\n",
    "data_all = data_all.rename(columns={\"score_Original\":\"score_original\", \"score_Flipped\":\"score_flipped\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "82eb1024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: ar  # 1100  samples before taking intersection of models. \n",
      "Language: ar kept # 904  samples (after intersection between all models). \n",
      "Language: de  # 1100  samples before taking intersection of models. \n",
      "Language: de kept # 913  samples (after intersection between all models). \n",
      "Language: es  # 1096  samples before taking intersection of models. \n",
      "Language: es kept # 1002  samples (after intersection between all models). \n",
      "Language: fr  # 1099  samples before taking intersection of models. \n",
      "Language: fr kept # 968  samples (after intersection between all models). \n",
      "Language: hi  # 1098  samples before taking intersection of models. \n",
      "Language: hi kept # 969  samples (after intersection between all models). \n",
      "Language: it  # 1094  samples before taking intersection of models. \n",
      "Language: it kept # 954  samples (after intersection between all models). \n",
      "Language: pt  # 1089  samples before taking intersection of models. \n",
      "Language: pt kept # 985  samples (after intersection between all models). \n",
      "Language: ru  # 1100  samples before taking intersection of models. \n",
      "Language: ru kept # 965  samples (after intersection between all models). \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>context</th>\n",
       "      <th>source</th>\n",
       "      <th>reference_original</th>\n",
       "      <th>reference_flipped</th>\n",
       "      <th>score_original</th>\n",
       "      <th>score_flipped</th>\n",
       "      <th>model</th>\n",
       "      <th>lang</th>\n",
       "      <th>src_gender</th>\n",
       "      <th>type</th>\n",
       "      <th>score_M</th>\n",
       "      <th>score_F</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Slap-guitarist Miyavi, who has his roots in vi...</td>\n",
       "      <td>It was the most successful international tour ...</td>\n",
       "      <td>كانت من أنجح الرحلات الدولية التي نفذها فنان ي...</td>\n",
       "      <td>كانت من أنجح الرحلات الدولية التي نفذتها فنانة...</td>\n",
       "      <td>0.807939</td>\n",
       "      <td>0.806999</td>\n",
       "      <td>Kiwi 22--translated ctx--GT</td>\n",
       "      <td>ar</td>\n",
       "      <td>male</td>\n",
       "      <td>neural_metric</td>\n",
       "      <td>0.807939</td>\n",
       "      <td>0.806999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Whittier was heavily influenced by the doctrin...</td>\n",
       "      <td>Whittier was first introduced to poetry by a t...</td>\n",
       "      <td>تعرّف ويتري على الشعر لأول مرة عن طريق معلم.</td>\n",
       "      <td>تعرّفت ويتري على الشعر لأول مرة عن طريق معلم.</td>\n",
       "      <td>0.844404</td>\n",
       "      <td>0.827781</td>\n",
       "      <td>Kiwi 22--translated ctx--GT</td>\n",
       "      <td>ar</td>\n",
       "      <td>male</td>\n",
       "      <td>neural_metric</td>\n",
       "      <td>0.844404</td>\n",
       "      <td>0.827781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>He was later appointed as a Civil, Commercial ...</td>\n",
       "      <td>Guinle entered politics as Government Secretar...</td>\n",
       "      <td>دخل Guinle السياسة كسكرتير المحافظة لبلدية Com...</td>\n",
       "      <td>دخلت Guinle السياسة كسكرتير المحافظة لبلدية Co...</td>\n",
       "      <td>0.776866</td>\n",
       "      <td>0.764903</td>\n",
       "      <td>Kiwi 22--translated ctx--GT</td>\n",
       "      <td>ar</td>\n",
       "      <td>male</td>\n",
       "      <td>neural_metric</td>\n",
       "      <td>0.776866</td>\n",
       "      <td>0.764903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                            context  \\\n",
       "2   2  Slap-guitarist Miyavi, who has his roots in vi...   \n",
       "3   3  Whittier was heavily influenced by the doctrin...   \n",
       "5   5  He was later appointed as a Civil, Commercial ...   \n",
       "\n",
       "                                              source  \\\n",
       "2  It was the most successful international tour ...   \n",
       "3  Whittier was first introduced to poetry by a t...   \n",
       "5  Guinle entered politics as Government Secretar...   \n",
       "\n",
       "                                  reference_original  \\\n",
       "2  كانت من أنجح الرحلات الدولية التي نفذها فنان ي...   \n",
       "3       تعرّف ويتري على الشعر لأول مرة عن طريق معلم.   \n",
       "5  دخل Guinle السياسة كسكرتير المحافظة لبلدية Com...   \n",
       "\n",
       "                                   reference_flipped  score_original  \\\n",
       "2  كانت من أنجح الرحلات الدولية التي نفذتها فنانة...        0.807939   \n",
       "3      تعرّفت ويتري على الشعر لأول مرة عن طريق معلم.        0.844404   \n",
       "5  دخلت Guinle السياسة كسكرتير المحافظة لبلدية Co...        0.776866   \n",
       "\n",
       "   score_flipped                        model lang src_gender           type  \\\n",
       "2       0.806999  Kiwi 22--translated ctx--GT   ar       male  neural_metric   \n",
       "3       0.827781  Kiwi 22--translated ctx--GT   ar       male  neural_metric   \n",
       "5       0.764903  Kiwi 22--translated ctx--GT   ar       male  neural_metric   \n",
       "\n",
       "    score_M   score_F  \n",
       "2  0.807939  0.806999  \n",
       "3  0.844404  0.827781  \n",
       "5  0.776866  0.764903  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "final_df = list()\n",
    "original_shapes = {}\n",
    "\n",
    "for (lang, model), subdf in data_all.groupby([\"lang\", \"model\"]):\n",
    "    \n",
    "    original_shape = subdf.shape\n",
    "    # print(lang,model ,original_shape)\n",
    "    if lang not in original_shapes:\n",
    "        original_shapes[lang]=original_shape\n",
    "        \n",
    "    # print(lang, model, original_shape)\n",
    "    filtered = subdf.copy()\n",
    "final_df = list()\n",
    "original_shapes = {}\n",
    "\n",
    "for (lang, model), subdf in data_all.groupby([\"lang\", \"model\"]):\n",
    "    \n",
    "    original_shape = subdf.shape\n",
    "    # print(lang,model ,original_shape)\n",
    "    if lang not in original_shapes:\n",
    "        original_shapes[lang]=original_shape\n",
    "        \n",
    "    # print(lang, model, original_shape)\n",
    "    filtered = subdf.copy()\n",
    "    filtered = filtered.dropna(subset=[\"score_original\", \"score_flipped\"])\n",
    "    # rescale scores of LLMs from 0-100 to 0-1\n",
    "    if filtered.iloc[0][\"type\"] == \"LLM\":\n",
    "        filtered[\"score_original\"] = filtered[\"score_original\"] / 100\n",
    "        filtered[\"score_flipped\"] = filtered[\"score_flipped\"] / 100\n",
    "    \n",
    "        # filter out all rows that have scores outside the range [0, 1]\n",
    "        filtered = filtered.loc[\n",
    "            (filtered[\"score_original\"] > 0.1) & \\\n",
    "            (filtered[\"score_original\"] <= 1) & \\\n",
    "            (filtered[\"score_flipped\"] > 0.1) & \\\n",
    "            (filtered[\"score_flipped\"] <= 1)\n",
    "        ]\n",
    "    elif \"MetricX\" in filtered.iloc[0][\"model\"]:\n",
    "        filtered[\"score_original\"] = 1 - filtered[\"score_original\"] / 25\n",
    "        filtered[\"score_flipped\"] = 1 - filtered[\"score_flipped\"] / 25\n",
    "    else:\n",
    "        #no filtering needed for classical neural metrics!\n",
    "        pass\n",
    "    final_df.append(filtered)\n",
    "\n",
    "data_all = pd.concat(final_df).copy()\n",
    "\n",
    "\n",
    "# Compute the intersection of rows to be kept for each language\n",
    "keep_rows_inter = {}\n",
    "for lang, lang_df in data_all.groupby(\"lang\"):\n",
    "    common_rows = set(lang_df.index)\n",
    "    for model, model_df in lang_df.groupby(\"model\"):\n",
    "        common_rows.intersection_update(set(model_df.index))\n",
    "    keep_rows_inter[lang] = list(common_rows)\n",
    "\n",
    "\n",
    "for lang in keep_rows_inter:\n",
    "    print(f\"Language: {lang}  # {original_shapes[lang][0]}  samples before taking intersection of models. \")\n",
    "    print(f\"Language: {lang} kept # {len(keep_rows_inter[lang])}  samples (after intersection between all models). \")\n",
    "\n",
    "# then we filter again!\n",
    "final_df=[]\n",
    "removed_info={lang:{} for lang in LANGS}\n",
    "\n",
    "for (lang, model), subdf in data_all.groupby([\"lang\", \"model\"]):\n",
    "    filtered=subdf.copy()\n",
    "    filtered = filtered.loc[keep_rows_inter[lang]]\n",
    "    removed_info[lang][model] =  100 * (1 - filtered.shape[0] / original_shapes[lang][0])\n",
    "\n",
    "    final_df.append(filtered)\n",
    "data_all = pd.concat(final_df).copy()\n",
    "data_all[\"score_M\"] = data_all.apply(lambda x: x[\"score_original\"] if x[\"src_gender\"] == \"male\" else x[\"score_flipped\"], axis=1)\n",
    "data_all[\"score_F\"] = data_all.apply(lambda x: x[\"score_original\"] if x[\"src_gender\"] == \"female\" else x[\"score_flipped\"], axis=1)\n",
    "\n",
    "\n",
    "#we remove the standard ctx models + all llms \n",
    "data_all = data_all[~data_all['model'].str.contains('original ctx', case=True, na=False)]\n",
    "data_all = data_all[~data_all['type'].str.contains('LLM', case=True, na=False)]\n",
    "\n",
    "data_all.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c41a816d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Kiwi 22--translated ctx--GT',\n",
       "       'Kiwi 22--translated ctx--NLLB 3.3B',\n",
       "       'Kiwi 23 XL--translated ctx--GT',\n",
       "       'Kiwi 23 XL--translated ctx--NLLB 3.3B',\n",
       "       'Kiwi 23 XXL--translated ctx--GT',\n",
       "       'Kiwi 23 XXL--translated ctx--NLLB 3.3B',\n",
       "       'MetricX23 LARGE--translated ctx--GT',\n",
       "       'MetricX23 LARGE--translated ctx--NLLB 3.3B',\n",
       "       'MetricX23 XL--translated ctx--GT',\n",
       "       'MetricX23 XL--translated ctx--NLLB 3.3B',\n",
       "       'xCOMET XL--translated ctx--GT',\n",
       "       'xCOMET XL--translated ctx--NLLB 3.3B',\n",
       "       'xCOMET XXL--translated ctx--GT',\n",
       "       'xCOMET XXL--translated ctx--NLLB 3.3B'], dtype=object)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_all.model.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "49d2f1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kiwi 22--translated ctx--GT ar\n",
      "Kiwi 22--translated ctx--GT de\n",
      "Kiwi 22--translated ctx--GT es\n",
      "Kiwi 22--translated ctx--GT fr\n",
      "Kiwi 22--translated ctx--GT hi\n",
      "Kiwi 22--translated ctx--GT it\n",
      "Kiwi 22--translated ctx--GT pt\n",
      "Kiwi 22--translated ctx--GT ru\n",
      "Kiwi 22--translated ctx--NLLB 3.3B ar\n",
      "Kiwi 22--translated ctx--NLLB 3.3B de\n",
      "Kiwi 22--translated ctx--NLLB 3.3B es\n",
      "Kiwi 22--translated ctx--NLLB 3.3B fr\n",
      "Kiwi 22--translated ctx--NLLB 3.3B hi\n",
      "Kiwi 22--translated ctx--NLLB 3.3B it\n",
      "Kiwi 22--translated ctx--NLLB 3.3B pt\n",
      "Kiwi 22--translated ctx--NLLB 3.3B ru\n",
      "Kiwi 23 XL--translated ctx--GT ar\n",
      "Kiwi 23 XL--translated ctx--GT de\n",
      "Kiwi 23 XL--translated ctx--GT es\n",
      "Kiwi 23 XL--translated ctx--GT fr\n",
      "Kiwi 23 XL--translated ctx--GT hi\n",
      "Kiwi 23 XL--translated ctx--GT it\n",
      "Kiwi 23 XL--translated ctx--GT pt\n",
      "Kiwi 23 XL--translated ctx--GT ru\n",
      "Kiwi 23 XL--translated ctx--NLLB 3.3B ar\n",
      "Kiwi 23 XL--translated ctx--NLLB 3.3B de\n",
      "Kiwi 23 XL--translated ctx--NLLB 3.3B es\n",
      "Kiwi 23 XL--translated ctx--NLLB 3.3B fr\n",
      "Kiwi 23 XL--translated ctx--NLLB 3.3B hi\n",
      "Kiwi 23 XL--translated ctx--NLLB 3.3B it\n",
      "Kiwi 23 XL--translated ctx--NLLB 3.3B pt\n",
      "Kiwi 23 XL--translated ctx--NLLB 3.3B ru\n",
      "Kiwi 23 XXL--translated ctx--GT ar\n",
      "Kiwi 23 XXL--translated ctx--GT de\n",
      "Kiwi 23 XXL--translated ctx--GT es\n",
      "Kiwi 23 XXL--translated ctx--GT fr\n",
      "Kiwi 23 XXL--translated ctx--GT hi\n",
      "Kiwi 23 XXL--translated ctx--GT it\n",
      "Kiwi 23 XXL--translated ctx--GT pt\n",
      "Kiwi 23 XXL--translated ctx--GT ru\n",
      "Kiwi 23 XXL--translated ctx--NLLB 3.3B ar\n",
      "Kiwi 23 XXL--translated ctx--NLLB 3.3B de\n",
      "Kiwi 23 XXL--translated ctx--NLLB 3.3B es\n",
      "Kiwi 23 XXL--translated ctx--NLLB 3.3B fr\n",
      "Kiwi 23 XXL--translated ctx--NLLB 3.3B hi\n",
      "Kiwi 23 XXL--translated ctx--NLLB 3.3B it\n",
      "Kiwi 23 XXL--translated ctx--NLLB 3.3B pt\n",
      "Kiwi 23 XXL--translated ctx--NLLB 3.3B ru\n",
      "MetricX23 LARGE--translated ctx--GT ar\n",
      "MetricX23 LARGE--translated ctx--GT de\n",
      "MetricX23 LARGE--translated ctx--GT es\n",
      "MetricX23 LARGE--translated ctx--GT fr\n",
      "MetricX23 LARGE--translated ctx--GT hi\n",
      "MetricX23 LARGE--translated ctx--GT it\n",
      "MetricX23 LARGE--translated ctx--GT pt\n",
      "MetricX23 LARGE--translated ctx--GT ru\n",
      "MetricX23 LARGE--translated ctx--NLLB 3.3B ar\n",
      "MetricX23 LARGE--translated ctx--NLLB 3.3B de\n",
      "MetricX23 LARGE--translated ctx--NLLB 3.3B es\n",
      "MetricX23 LARGE--translated ctx--NLLB 3.3B fr\n",
      "MetricX23 LARGE--translated ctx--NLLB 3.3B hi\n",
      "MetricX23 LARGE--translated ctx--NLLB 3.3B it\n",
      "MetricX23 LARGE--translated ctx--NLLB 3.3B pt\n",
      "MetricX23 LARGE--translated ctx--NLLB 3.3B ru\n",
      "MetricX23 XL--translated ctx--GT ar\n",
      "MetricX23 XL--translated ctx--GT de\n",
      "MetricX23 XL--translated ctx--GT es\n",
      "MetricX23 XL--translated ctx--GT fr\n",
      "MetricX23 XL--translated ctx--GT hi\n",
      "MetricX23 XL--translated ctx--GT it\n",
      "MetricX23 XL--translated ctx--GT pt\n",
      "MetricX23 XL--translated ctx--GT ru\n",
      "MetricX23 XL--translated ctx--NLLB 3.3B ar\n",
      "MetricX23 XL--translated ctx--NLLB 3.3B de\n",
      "MetricX23 XL--translated ctx--NLLB 3.3B es\n",
      "MetricX23 XL--translated ctx--NLLB 3.3B fr\n",
      "MetricX23 XL--translated ctx--NLLB 3.3B hi\n",
      "MetricX23 XL--translated ctx--NLLB 3.3B it\n",
      "MetricX23 XL--translated ctx--NLLB 3.3B pt\n",
      "MetricX23 XL--translated ctx--NLLB 3.3B ru\n",
      "xCOMET XL--translated ctx--GT ar\n",
      "xCOMET XL--translated ctx--GT de\n",
      "xCOMET XL--translated ctx--GT es\n",
      "xCOMET XL--translated ctx--GT fr\n",
      "xCOMET XL--translated ctx--GT hi\n",
      "xCOMET XL--translated ctx--GT it\n",
      "xCOMET XL--translated ctx--GT pt\n",
      "xCOMET XL--translated ctx--GT ru\n",
      "xCOMET XL--translated ctx--NLLB 3.3B ar\n",
      "xCOMET XL--translated ctx--NLLB 3.3B de\n",
      "xCOMET XL--translated ctx--NLLB 3.3B es\n",
      "xCOMET XL--translated ctx--NLLB 3.3B fr\n",
      "xCOMET XL--translated ctx--NLLB 3.3B hi\n",
      "xCOMET XL--translated ctx--NLLB 3.3B it\n",
      "xCOMET XL--translated ctx--NLLB 3.3B pt\n",
      "xCOMET XL--translated ctx--NLLB 3.3B ru\n",
      "xCOMET XXL--translated ctx--GT ar\n",
      "xCOMET XXL--translated ctx--GT de\n",
      "xCOMET XXL--translated ctx--GT es\n",
      "xCOMET XXL--translated ctx--GT fr\n",
      "xCOMET XXL--translated ctx--GT hi\n",
      "xCOMET XXL--translated ctx--GT it\n",
      "xCOMET XXL--translated ctx--GT pt\n",
      "xCOMET XXL--translated ctx--GT ru\n",
      "xCOMET XXL--translated ctx--NLLB 3.3B ar\n",
      "xCOMET XXL--translated ctx--NLLB 3.3B de\n",
      "xCOMET XXL--translated ctx--NLLB 3.3B es\n",
      "xCOMET XXL--translated ctx--NLLB 3.3B fr\n",
      "xCOMET XXL--translated ctx--NLLB 3.3B hi\n",
      "xCOMET XXL--translated ctx--NLLB 3.3B it\n",
      "xCOMET XXL--translated ctx--NLLB 3.3B pt\n",
      "xCOMET XXL--translated ctx--NLLB 3.3B ru\n"
     ]
    }
   ],
   "source": [
    "\n",
    "acc_list2 = list()\n",
    "for (model, lang), subdf in data_all.groupby([\"model\",\"lang\"]):\n",
    "    print(model,lang)\n",
    "    acc = eval_metrics.Accuracy({\"F\": np.array(subdf[\"score_F\"]), \"M\": np.array(subdf[\"score_M\"]),\"y_true\":np.array(subdf[\"src_gender\"])}).score()\n",
    "    group_metrics = eval_metrics.GroupMetricsBootstraping({\"F\": np.array(subdf[\"score_F\"]), \"M\": np.array(subdf[\"score_M\"]),\"y_true\":np.array(subdf[\"src_gender\"])},alternative=\"greater\").stat_significance_with_paired_bootstrap()\n",
    "    \n",
    "    d = dict(\n",
    "        model=model,\n",
    "        lang=lang,\n",
    "        acc_total = acc,\n",
    "        error_rate_total = group_metrics[\"results\"][\"total_error_rate\"],\n",
    "        error_rate_male = group_metrics[\"results\"][\"male\"][\"fnr\"],\n",
    "        error_rate_fem = group_metrics[\"results\"][\"female\"][\"fnr\"],\n",
    "        error_rate_diff =    group_metrics[\"results\"][\"fnr_diff\"],\n",
    "        error_rate_ratio =  group_metrics[\"results\"][\"fnr_ratio\"],\n",
    "        stat_significance = group_metrics[\"stat_significance\"],\n",
    "        p_value = group_metrics[\"p_value\"],\n",
    "    )\n",
    "    acc_list2.append(d)\n",
    "\n",
    "acc_df2 = pd.DataFrame(acc_list2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cb667364",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('./results-copied/stats/nonambiguous-contextual/translated_ctx_comparison', exist_ok=True)\n",
    "acc_df_path = './results-copied/stats/nonambiguous-contextual/translated_ctx_comparison/results.csv'\n",
    "acc_df2.to_csv(acc_df_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a0c106",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
